{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Blackfish","text":"<p>Blackfish is an open source \"ML-as-a-Service\" (MLaaS) platform that helps researchers use state-of-the-art, open source machine learning and  artificial intelligence models. With Blackfish, researchers can spin up their own version of popular public cloud services (e.g., ChatGPT, Amazon Transcribe, etc.) using high-performance computing (HPC) resources already available on campus.</p> <p>The primary goal of Blackfish is to facilitate transparent, reproducible, and safe research using advanced machine learning and artificial intelligence. We do this by providing mechanisms to run open source models with user-defined configurations in isolated containers.</p> <p>For academic research, open source models present several advantages over closed source models. First, whereas large-scale projects using public cloud services might cost $10K to $100K for similar quality results, open source models running on HPC resources are free to researchers. Second, with open source models you know exactly what model you are using and you can easily provide a copy of that model to other researchers. Closed source models can and do change without notice. Third, using open-source models allows complete transparency into how your data is being used.</p>"},{"location":"#why-blackfish","title":"Why Blackfish?","text":""},{"location":"#1-its-easy","title":"1. It's easy! \ud83e\udd73","text":"<p>Researchers should focus on research, not tooling. We try to meet researchers where they're at by providing multiple ways to work with Blackfish, including a CLI and browser-based UI. Don't want to install Python packages? Ask your HPC admins about adding Blackfish to your Open OnDemand portal!</p>"},{"location":"#2-its-transparent","title":"2. It's transparent \ud83d\udd0d","text":"<p>You decide what model to run (down to the Git commit) and how you want it configured, so there are no unexpected (or undetected) changes in performance. Services are run in isolated containers with on-premise compute and fully transparent code so you know exactly how your data is being handled.</p>"},{"location":"#3-its-free","title":"3. It's free! \ud83d\udcb8","text":"<p>You have an HPC cluster. We have software to run on it.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#step-1-install-blackfish","title":"Step 1 - Install blackfish","text":"<pre><code>pip install blackfish-ml\n</code></pre>"},{"location":"#step-2-create-a-profile","title":"Step 2 - Create a profile","text":"<pre><code>blackfish init\n\n# Example responses\n# &gt; name: default\n# &gt; type: slurm\n# &gt; host: della.princeton.edu\n# &gt; user: timmy\n# &gt; home: /home/timmy123/.blackfish\n# &gt; cache: /scratch/gpfs/shared/.blackfish\n</code></pre>"},{"location":"#step-3-start-the-api","title":"Step 3 - Start the API","text":"<pre><code>blackfish start\n</code></pre>"},{"location":"#step-4-obtain-a-model","title":"Step 4 - Obtain a model","text":"<pre><code># This will take awhile...\nblackfish model add --profile default openai/whisper-large-v3\n</code></pre>"},{"location":"#step-5-run-a-service","title":"Step 5 - Run a service","text":"<pre><code>blackfish run --profile default --mount $HOME/Downloads speech-recognition openai/whisper-tiny\n</code></pre>"},{"location":"#step-6-submit-a-request","title":"Step 6 - Submit a request","text":"<pre><code># First, check the service status...\nblackfish ls\n\n# Once the service is healthy...\ncurl -X POST 'http://localhost:8080/transcribe' -H 'Content-Type: application/json' -d '{\"audio_path\": \"/data/audio/NY045.mp3\", \"response_format\": \"json\"}'\n</code></pre>"},{"location":"#details","title":"Details","text":"<p>Blackfish consists of three primary components: a core API (\"Blackfish API\"), a command-line interface (\"Blackfish CLI\") and a browser-based user interface (\"Blackfish UI\"). The Blackfish API performs all key operations while the Blackfish CLI and UI provide convenient methods for interacting with the Blackfish API. Essentially, the Blackfish API automates the process of hosting AI models as APIs. Users instruct the Blackfish API\u2014directly or via an interface\u2014to deploy a model and the Blackfish API creates a \"service API\" running that model. The researcher that starts a service \"owns\" that service: she has exclusive access to its use and the resources (e.g., CPU and GPU memory) required to deploy it. Blackfish tracks the status of users' services and provides methods to stop services when they are no longer needed.</p> <p>In general, service APIs do not run on the same machine as the Blackfish application. Thus, when a researcher requests a model, she must specify a host for the service. The service API runs on the specifieid host and Blackfish ensures that the interface is able to communicate with the remote service API. There are several ways for researchers to setup and use Blackfish depending on their requirements. For testing and development purposes, users can run everything on their laptop, but his option is only practical for models with light resource requirements. Typically, users will want to run services on high-performance GPUs available on an HPC cluster with a Slurm job scheduler. In that case, researchers can run the Blackfish API on their local laptop or on the HPC cluster.</p> <p>Note</p> <p>Blackfish doesn't synchronize application data across machines. Services started by an instance of Blackfish running on your laptop will not show up on an HPC cluster. However, job data for services initiated by your laptop will be stored on the remote cluster.</p> <p></p> <p>Figure The Blackfish architecture for running remote service APIs on a Slurm cluster.</p>"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#python","title":"Python","text":"<p>Blackfish requires Python to run locally. Alternatively, Blackfish can be added to your university's Open OnDemand portal, which allows users to run applications on HPC resources through a web browser. For more information, see our companion repo blackfish-ondemand.</p>"},{"location":"#docker-apptainer","title":"Docker &amp; Apptainer","text":"<p>Blackfish uses Docker to run service containers locally. Services run on HPC clusters rely on Apptainer.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Blackfish is maintained by research software engineers at Princeton University's Data Driven Social Science Initiative.</p> <ol> <li> <p>Support is currently limited to clusters running the Slurm job manager.\u00a0\u21a9</p> </li> <li> <p>Inference results may not be \"exactly reproducible\"\u2014i.e., generating same outputs from same inputs\u2014depending on the details of the model and inference settings. Blackfish allows researchers to \"reproduce\" findings in the sense of running the exact same model with the exact same settings.\u00a0\u21a9</p> </li> </ol>"},{"location":"admin/","title":"Platform Administration","text":""},{"location":"admin/#application-management","title":"Application Management","text":"<p>Blackfish is Litestar application that is managed using the <code>litestar</code> CLI. You can get help with <code>litestar</code> by running <code>litestar --help</code> at the command line from within the application's home directory. Below are some of the essential tasks.</p>"},{"location":"admin/#run-the-application","title":"Run the application","text":"<pre><code>litestar run  --reload # refresh updates during development\n</code></pre>"},{"location":"admin/#run-a-database-migration","title":"Run a database migration","text":"<p>First, check where your current migration: <pre><code>litestar database show-current-revision\n</code></pre> Make some updates to the database models, then run <pre><code>litestar make-migration \"a new migration\"\n</code></pre> to create a new migration.</p> <p>Finally, check that the auto-generated migration file looks correct and run <pre><code>litestar database upgrade\n</code></pre></p>"},{"location":"admin/#pull-a-container-image","title":"Pull a container image","text":""},{"location":"admin/#apptainer","title":"Apptainer","text":"<p>Services deployed on high-performance computing systems need to be run by Apptainer instead of Docker. Apptainer will not run Docker images directly. Instead, you need to convert Docker images to SIF files. For images hosted on Docker Hub, running <code>apptainer pull</code> will do this automatically. For example,</p> <pre><code>apptainer pull docker://ghcr.io/huggingface/text-generation-inference:latest\n</code></pre> <p>This command generates a file <code>text-generation-inference_latest.sif</code>. In order for users of the remote to access the image, it should be moved to a shared cache directory, e.g., <code>/scratch/gpfs/.blackfish/images</code>.</p>"},{"location":"admin/#download-a-model-snapshot","title":"Download a model snapshot","text":""},{"location":"admin/#hugging-face","title":"Hugging Face","text":"<p>Models should generally be pulled from the Hugging Face model hub. This can be done by either visiting the web page for the model card or using of one Hugging Face's Python packages. The latter is preferred as it stores files in a consistent manner in the cache directory. E.g., <pre><code>from transformers import pipeline\npipeline(\n    task='text-generation',\n    model='meta-llama/Meta-Llama-3-8B',\n    token=&lt;token&gt;,\n    revision=&lt;revision&gt;,\n\n)\n# or\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\nmodel = AutoModelForCausalLM('meta-llama/Meta-Llama-3-8b')\n# or\nfrom huggingface_hub import shapshot_download\nsnapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B\")\n</code></pre> These commands store models files to <code>~/.cache/huggingface/hub/</code> by default. You can modify the directory by setting <code>HF_HOME</code> in the local environment or providing a <code>cache_dir</code> argument (where applicable). After the model files are downloaded, they should be moved to a shared cache directory, e.g., <code>/scratch/gpfs/blackfish/models</code>, and permissions on the new model directory should be updated to <code>755</code> (recursively) to allow all users read and execute.</p> <p>Note</p> <p>Users can only download new snapshots to <code>profile.home_dir</code>. Thus, if a model is found before running a service, then the image should look for model data in whichever cache directory the snapshot is found. Otherwise, the service should bind to <code>profile.home_dir</code> so that model files are stored there. Users should not be given write access to <code>profile.cache_dir</code>. If a user does not specify a revision, then we need to make sure that the image doesn't try to download a different revision in the case that a version of the requested model already exists in <code>profile.cache_dir</code> because this directory is assumed to be read-only and the Docker image might try to download a different revision.</p>"},{"location":"develop/","title":"Developer Guide","text":""},{"location":"develop/#setup","title":"Setup","text":""},{"location":"develop/#ssh","title":"SSH","text":"<p>Running Blackfish from your laptop to start remote services requires a seamless (i.e., password-less) method of communication with remote clusters. A simple to set up password-less login is with the <code>ssh-keygen</code> and <code>ssh-copy-id</code> utilitites.</p> <p>First, make sure that you are connected to your institution's network (or VPN, if required). Then, type the following at the command-line: <pre><code>ssh-keygen -t rsa # generates ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa\nssh-copy-id &lt;user&gt;@&lt;host&gt; # answer yes to transfer the public key\n</code></pre> These commands create a secure public-private key pair and send the public key to the HPC server. You now have password-less access to your HPC server!</p>"},{"location":"develop/#api","title":"API","text":"<p>Blackfish is Litestar application that is managed using the <code>litestar</code> CLI. You can get help with <code>litestar</code> by running <code>litestar --help</code> at the command line from within the application's home directory. Below are some of the essential tasks.</p>"},{"location":"develop/#litestar-commands","title":"Litestar Commands","text":""},{"location":"develop/#run","title":"Run","text":"<pre><code>litestar run  # add --reload to automatically refresh updates during development\n</code></pre>"},{"location":"develop/#database","title":"Database","text":"<pre><code># First, check where your current migration:\nlitestar database show-current-revision\n# Make some updates to the database models, then:\nlitestar database make-migration \"a new migration\"  # create a new migration\n# check that the auto-generated migration file looks correct, then:\nlitestar database upgrade\n</code></pre>"},{"location":"develop/#configuration","title":"Configuration","text":"<p>The application and command-line interface (CLI) pull their settings from environment variables and/or (for the application) arguments provided at start-up. The environment variables include: <pre><code>HOST = \"localhost\"\nPORT = 8000\nSTATIC_DIR = \"/Users/colinswaney/GitHub/blackfish/src\" # source of static files\nHOME_DIR = \"/Users/colinswaney/.blackfish\" # source of application data\nDEBUG = true # log debug statements\nDEV_MODE = true # run server in development mode (no auth)\nCONTAINER_PROVIDER = \"docker\" # determines how to launch containers\n</code></pre></p>"},{"location":"develop/#images","title":"Images","text":""},{"location":"develop/#obtaining-apptainer-images","title":"Obtaining Apptainer images","text":"<p>Services deployed on high-performance computing systems need to be run by Apptainer instead of Docker. Apptainer will not run Docker images directly. Instead, you need to convert Docker images to SIF files. For images hosted on Docker Hub, running <code>apptainer pull</code> will do this automatically. For example,</p> <pre><code>apptainer pull docker://ghcr.io/huggingface/text-generation-inference:latest\n</code></pre> <p>This command generates a file <code>text-generation-inference_latest.sif</code>. In order for users of the remote to access the image, it should be moved to a shared cache directory, e.g., <code>/scratch/gpfs/.blackfish/images</code>.</p>"},{"location":"develop/#models","title":"Models","text":""},{"location":"develop/#obtaining-models","title":"Obtaining models","text":"<p>Update The recommended method to manage models is now via the <code>blackfish model</code> commands using a profile linked to the shared cache directory (make sure to use the <code>--use_cache</code> flag). This will ensure that <code>info.json</code> files are updated. If the shared cache permissions have been set properly, then there should be no need to update permissions on the newly added files.</p> <p>Models should generally be pulled from the Hugging Face model hub. This can be done by either visiting the web page for the model card or using of one Hugging Face's Python packages. The latter is preferred as it stores files in a consistent manner in the cache directory. E.g., <pre><code>from transformers import pipeline\npipeline(\n    task='text-generation',\n    model='meta-llama/Meta-Llama-3-8B',\n    token=&lt;token&gt;,\n    revision=&lt;revision&gt;,\n\n)\n# or\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\nmodel = AutoModelForCausalLM('meta-llama/Meta-Llama-3-8b')\n# or\nfrom huggingface_hub import shapshot_download\nsnapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B\")\n</code></pre> These commands store models files to <code>~/.cache/huggingface/hub/</code> by default. You can modify the directory by setting <code>HF_HOME</code> in the local environment or providing a <code>cache_dir</code> argument (where applicable). After the model files are downloaded, they should be moved to a shared cache directory, e.g., <code>/scratch/gpfs/blackfish/models</code>, and permissions on the new model directory should be updated to <code>755</code> (recursively) to allow all users read and execute.</p>"},{"location":"develop/#ui","title":"UI","text":""},{"location":"develop/#updating-build","title":"Updating <code>build</code>","text":"<p>Blackfish ships with a copy of the built user interface so that users can run the user interface with having to install <code>npm</code>. To update the UI, you need:</p> <ol> <li>Build the UI Run <code>npm run build</code> in the <code>blackfish-ui</code> repo. The output of this command will be in <code>build/out</code>: <pre><code>\u279c tree build -d 1\nbuild\n\u2514\u2500\u2500 out\n    \u251c\u2500\u2500 _next\n    \u2502   \u251c\u2500\u2500 ssm_XfrOvugkYGVtNQ8ps\n    \u2502   \u2514\u2500\u2500 static\n    \u2502       \u251c\u2500\u2500 chunks\n    \u2502       \u2502   \u251c\u2500\u2500 app\n    \u2502       \u2502   \u2502   \u251c\u2500\u2500 _not-found\n    \u2502       \u2502   \u2502   \u251c\u2500\u2500 dashboard\n    \u2502       \u2502   \u2502   \u251c\u2500\u2500 login\n    \u2502       \u2502   \u2502   \u251c\u2500\u2500 speech-recognition\n    \u2502       \u2502   \u2502   \u2514\u2500\u2500 text-generation\n    \u2502       \u2502   \u2514\u2500\u2500 pages\n    \u2502       \u251c\u2500\u2500 css\n    \u2502       \u251c\u2500\u2500 media\n    \u2502       \u2514\n</code></pre></li> <li> <p>Copy <code>blackfish-ui/build/out</code> to <code>blackfish/src/build</code> <pre><code>cp -R build/out/* ~/GitHub/blackfish/src/build\n</code></pre></p> </li> <li> <p>Commit the change <pre><code>git add .\ngit commit\n# Add a useful message that includes the head of the UI, e.g.,\n# Update UI to blackfish-ui@7943376\n</code></pre></p> </li> </ol>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#pip","title":"pip","text":"<pre><code>python -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install blackfish\n</code></pre> <p>If everything worked, <code>which blackfish</code> should point to the installed command-line tool.</p>"},{"location":"getting_started/#setup","title":"Setup","text":"<p>There's a small amount of setup required before we get started with Blackfish. Fortunately, it's mostly automated.</p>"},{"location":"getting_started/#ssh","title":"SSH","text":"<p>Using Blackfish from your laptop requires a seamless (i.e., password-less) method of communicating with remote clusters. On many systems, this is simple to setup with the <code>ssh-keygen</code> and <code>ssh-copy-id</code> utilitites. First, make sure that you are connected to your institution's network (or VPN), then type the following at the command-line:</p> <pre><code>ssh-keygen -t rsa # generates ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa\nssh-copy-id &lt;user&gt;@&lt;host&gt; # answer yes to transfer the public key\n</code></pre> <p>These commands create a secure public-private key pair and send the public key to the HPC server you need access to. You now have password-less access to your HPC server!</p> <p>Warning</p> <p>Blackfish depends on seamless interaction with your university's HPC cluster. Before proceeding, make sure that you have enabled password-less login and are connected to your institutions network or VPN, if required.</p>"},{"location":"getting_started/#initialization","title":"Initialization","text":"<p>To initialize Blackfish, just type <pre><code>blackfish init\n</code></pre> and answer the prompts to create a new default profile.</p> <p>Note</p> <p>If your default profile connects to an HPC cluster, then Blackfish will attempt to set up the remote host at this point. Profile creation will fail if you're unable to connect to the HPC server and you'll need to re-run the <code>blackfish init</code> command or create a profile with <code>blackfish profile create</code> (see below).</p>"},{"location":"getting_started/#models-and-images","title":"Models and Images","text":"<p>Blackfish works best with locally available model files and container images. Having these files available locally allows Blackfish to avoid slow downloads during deployment. See the section on Obtaining Service Images and Models for more information, or talk to your institution's HPC cluster admins.</p>"},{"location":"getting_started/#configuration","title":"Configuration","text":"<p>The application and command-line interface (CLI) pull their settings from environment variables and/or (for the application) arguments provided at start-up. The most important environment variables are: <pre><code>BLACKFISH_HOST='localhost' # host for local instance of the Blackfish app\nBLACKFISH_PORT=8000 # port for local instance of the Blackfish app\nBLACKFISH_HOME_DIR='~/.blackfish' # location to store application data\nBLACKFISH_DEV_MODE=1 # run the application with development settings\nBLACKFISH_TOKEN='sealsaretasty' # a user-defined secret auth token\n</code></pre></p> <p>Running the application in development mode is recommended for development only on a shared system as it does not use authentication.</p>"},{"location":"getting_started/#profiles","title":"Profiles","text":"<p>The <code>blackfish profile</code> command provides methods for managing Blackfish profiles. Profiles are useful if you have access to multiple HPC resources or have multiple accounts on an HPC server. Each profile consists of some combination of the following attributes, depending on the profile type.</p> <p>Tip</p> <p>Blackfish profiles are stored in <code>$BLACKFISH_HOME/profiles.cfg</code>. On Linux, this is <code>$HOME/.blackfish/profiles.cfg</code> by default. You can modify this file directly, if needed, but you'll need to need setup any required remote resources by hand.</p>"},{"location":"getting_started/#schemas","title":"Schemas","text":"<p>Each profile specifies a number of attributes that allow Blackfish to find resources (e.g., model files) and deploy services accordingly. The exact attributes depend on the profile schema. There are currently two profile schemas: <code>LocalProfile</code> (\"local\") and <code>SlurmProfile</code> (\"slurm\"). All profiles require the following attributes:</p> <ul> <li><code>name</code>: the unique profile name. The \"default\" profile is used by Blackfish when a profile isn't explicitly provided.</li> <li><code>schema</code>: one of \"slurm\" or \"local\". The profile schema determines how services associated with this profile are deployed by Blackfish. Use \"slurm\" if this profile will run jobs on HPC and \"local\" to run jobs on your laptop (or wherever Blackfish is installed).</li> </ul> <p>The additional attribute requirements for specific types are listed below.</p>"},{"location":"getting_started/#slurm","title":"Slurm","text":"<p>A Slurm profile specifies how to schedule services on a (possibly) remote server (e.g., HPC cluster) running Slurm from a local machine.</p> <ul> <li><code>host</code>: a HPC server to run services on, e.g. <code>&lt;cluster&gt;@&lt;university&gt;.edu</code> or <code>localhost</code> (if running Blackfish on an  HPC cluster).</li> <li><code>user</code>: a user name on the HPC server.</li> <li><code>home</code>: a location on the HPC server to store application data, e.g., <code>/home/&lt;user&gt;/.blackfish</code></li> <li><code>cache</code>: a location on the HPC server to store additional (typically shared) model images and files. Blackfish does not attempt to create this directory for you, but it does require that it can be found.</li> </ul>"},{"location":"getting_started/#local","title":"Local","text":"<p>A local profile specifies how to run services on a local machine, i.e., your laptop or desktop, without a job scheduler. This is useful for development and for running models that do not require large amounts of resource, especially if the model is able to use the GPU on your laptop.</p> <ul> <li><code>home</code>: a location on the local machine to store application data, e.g., <code>/home/&lt;user&gt;/.blackfish</code></li> <li><code>cache</code>: a location on the local machine to store additional (typically shared) model images and files. Blackfish does not attempt to create this directory for you, but it does require that it can be found.</li> </ul>"},{"location":"getting_started/#commands","title":"Commands","text":""},{"location":"getting_started/#ls-list-profiles","title":"ls - List profiles","text":"<p>To view all profiles, type <pre><code>blackfish profile ls\n</code></pre></p>"},{"location":"getting_started/#add-create-a-profile","title":"add - Create a profile","text":"<p>Creating a new profile is as simple as typing <pre><code>blackfish profile add\n</code></pre></p> <p>and following the prompts (see attribute descriptions above). Note that profile names are unique.</p>"},{"location":"getting_started/#show-view-a-profile","title":"show - View a profile","text":"<p>You can view a list of all profiles with the <code>blackfish profile ls</code> command. If you want to view a specific profile, use the <code>blackfish profile show</code> command instead, e.g.</p> <pre><code>blackfish profile show --name &lt;profile&gt;\n</code></pre> <p>Leaving off the <code>--name</code> option above will display the default profile, which is used by most commands if no profile is explicitly provided.</p>"},{"location":"getting_started/#update-modify-a-profile","title":"update - Modify a profile","text":"<p>To modify a profile, use the <code>blackfish profile update</code> command, e.g.</p> <p><pre><code>blackfish profile update --name &lt;profile&gt;\n</code></pre> This command updates the default profile if not <code>--name</code> is specified. Note that you cannot change the name or type attributes of a profile.</p>"},{"location":"getting_started/#rm-delete-a-profile","title":"rm - Delete a profile","text":"<p>To delete a profile, type <code>blackfish profile rm --name &lt;profile&gt;</code>. By default, the command requires you to confirm before deleting. <pre><code>blackfish profile rm --name &lt;profile&gt;\n</code></pre></p>"},{"location":"getting_started/#usage","title":"Usage","text":"<p>Once you've initialized Blackfish and created a profile, you're ready to go. Their are two ways ways to interact with Blackfish: in a browser, via the user interface (UI), or at the command-line using the Blackfish CLI. In either case, the entrypoint is to type <pre><code>blackfish start\n</code></pre> in the command-line. If everything worked, you should see a message stating the application startup is complete.</p> <p>At this point, we need to decide how we want to interact with Blackfish. The UI is available in your browser by heading over to <code>http://localhost:8000</code>. It's a relatively straight-forward interface, and we have detailed usage examples on the user interface page, so let's instead take a look at the CLI.</p> <p>Open a new terminal tab or window. First, let's see what type of services are available. <pre><code>blackfish run --help\n\n Usage: blackfish run [OPTIONS] COMMAND [ARGS]...\n\n Run an inference service.\n The format of options approximately follows that of Slurm's `sbatch` command.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --time                 TEXT     The duration to run the service for, e.g., 1:00 (one hour).               \u2502\n\u2502 --ntasks-per-node      INTEGER  The number of tasks per compute node.                                     \u2502\n\u2502 --mem                  INTEGER  The memory required per compute node in GB, e.g., 16 (G).                 \u2502\n\u2502 --gres                 INTEGER  The number of GPU devices required per compute node, e.g., 1.             \u2502\n\u2502 --partition            TEXT     The HPC partition to run the service on.                                  \u2502\n\u2502 --constraint           TEXT     Required compute node features, e.g., 'gpu80'.                            \u2502\n\u2502 --profile          -p  TEXT     The Blackfish profile to use.                                             \u2502\n\u2502 --help                          Show this message and exit.                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 speech-recognition  Start a speech recognition service hosting MODEL with access to INPUT_DIR on the      \u2502\n\u2502                     service host. MODEL is specified as a repo ID, e.g., openai/whisper-tiny.             \u2502\n\u2502 text-generation     Start a text generation service hosting MODEL, where MODEL is specified as a repo ID, \u2502\n\u2502                     e.g., openai/whisper-tiny.                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> This command displays a list of available sub-commands. One of these is <code>text-generation</code>, which is a service that generates text given an input prompt. There are a variety of models that we might use to perform this task, so let's check out what's available on our setup.</p>"},{"location":"getting_started/#models","title":"Models","text":"<p><pre><code>blackfish model ls\nREPO                                   REVISION                                   PROFILE   IMAGE\nopenai/whisper-tiny                    169d4a4341b33bc18d8881c4b69c2e104e1cc0af   default   speech-recognition\nopenai/whisper-tiny                    be0ba7c2f24f0127b27863a23a08002af4c2c279   default   speech-recognition\nopenai/whisper-small                   973afd24965f72e36ca33b3055d56a652f456b4d   default   speech-recognition\nTinyLlama/TinyLlama-1.1B-Chat-v1.0     ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971   default   text-generation\nmeta-llama/Meta-Llama-3-70B            b4d08b7db49d488da3ac49adf25a6b9ac01ae338   macbook   text-generation\nopenai/whisper-tiny                    169d4a4341b33bc18d8881c4b69c2e104e1cc0af   macbook   speech-recognition\nTinyLlama/TinyLlama-1.1B-Chat-v1.0     4f42c91d806a19ae1a46af6c3fb5f4990d884cd6   macbook   text-generation\n</code></pre> As you can see, we have a number of models available.<sup>1</sup> Notice that <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> is listed twice. The first listing refers to a specific version of this model\u2014 <code>ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971</code>\u2014that is available to the <code>default</code> profile; the second listing refers to a different version (\"revision\") of the same model\u2014<code>4f42c91d806a19ae1a46af6c3fb5f4990d884cd6</code>\u2014that is available to the <code>macbook</code> profile. For reproducibility, it's important to keep track of the exact revision used.</p> <p>Let's go ahead and try to run one of these models.</p>"},{"location":"getting_started/#services","title":"Services","text":"<p>A service is a containerized API that is called to perform a specific task, such a text generation, using a model specified by the user when the API is created. Services perform inference in an \"online\" fashion, meaning that, in general, they process requests one input at a time. Users can create as many services as they like (and have resources to support) and interact with them simultaneously. Services are completely managed by the user: as the creator of a service, you are the only person that can stop or restart the service, and you control access to the service via an authentication token.</p>"},{"location":"getting_started/#commands_1","title":"Commands","text":""},{"location":"getting_started/#run-start-a-service","title":"<code>run</code> - Start a service","text":"<p>Looking back at the help message for <code>blackfish run</code>, we see that there are a few items that we should provide. First, we need to select the type of service to run. We've already decide to run <code>text-generation</code>, so we're good there. Next, there are a number of job options that we can provide. With the exception of <code>profile</code>, job options are based on the Slurm <code>sbatch</code> command and tell Blackfish the resources required to run a service. Finally, there are a number of \"container options\" available. To get a list of these, type <code>blackfish run text-generation --help</code>:</p> <p><pre><code>blackfish run text-generation --help\n\n Usage: blackfish run text-generation [OPTIONS] MODEL\n\n Start a text generation service hosting MODEL, where MODEL is specified as a repo ID, e.g.,\n openai/whisper-tiny.\n See https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/launcher for\n additional option details.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --name                    -n  TEXT     Assign a name to the service. A random name is assigned   \u2502\n\u2502                                        by default.                                               \u2502\n\u2502 --revision                -r  TEXT     Use a specific model revision. The most recent locally    \u2502\n\u2502                                        available (i.e., downloaded) revision is used by default. \u2502\n\u2502 --disable-custom-kernels               Disable custom CUDA kernels. Custom CUDA kernels are not  \u2502\n\u2502                                        guaranteed to run on all devices, but will run faster if  \u2502\n\u2502                                        they do.                                                  \u2502\n\u2502 --sharded                     TEXT     Shard the model across multiple GPUs. The API uses all    \u2502\n\u2502                                        available GPUs by default. Setting to 'true' with a       \u2502\n\u2502                                        single GPU results in an error.                           \u2502\n\u2502 --max-input-length            INTEGER  The maximum allowed input length (in tokens).             \u2502\n\u2502 --max-total-tokens            INTEGER  The maximum allowed total length of input and output (in  \u2502\n\u2502                                        tokens).                                                  \u2502\n\u2502 --dry-run                              Print the job script but do not run it.                   \u2502\n\u2502 --help                                 Show this message and exit.                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> The most important of these is the <code>revision</code>, which specifies the exact version of the model we want to run. By default, Blackfish selects the most recent locally available version. This container option (as well as <code>--name</code>) is available for all tasks: the remaining options are task-specific.</p> <p>We'll choose <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> for the required <code>MODEL</code> argument, which we saw earlier is available to the <code>default</code> and <code>macbook</code> profiles. This is a relatively small model, but we still want to ask for a GPU to speed things up. Putting it altogether, here's a command to start our service: <pre><code>blackfish run --profile della --gres 1 --mem 16 --ntasks-per-node 4 --time 00:30:00 --constraint 'amd&amp;gpu40' text-generation TinyLlama/TinyLlama-1.1B-Chat-v1.0 --api-key sealsaretasty\n\u2714 Found 49 models.\n\u2714 Found 1 snapshots.\n\u26a0 No revision provided. Using latest available commit: fe8a4ea1ffedaf415f4da2f062534de366a451e6.\n\u2714 Found model TinyLlama/TinyLlama-1.1B-Chat-v1.0!\n\u2714 Started service: fed36739-70b4-4dc4-8017-a4277563aef9\n</code></pre> What just happened? First, Blackfish checked to make sure that the requested model is available to the <code>della</code> profile. Next, it found a list of available revisions of the model and selected the most recently published version because no revision was specified. Finally, it sent a request to deploy the model. Helpfully, the CLI returned an ID associated with the new service <code>fed36739-70b4-4dc4-8017-a4277563aef9</code>, which we can use get information about our service via the <code>blackfish ls</code> command.</p> <p>Note</p> <p>If no <code>--revision</code> is provided, Blackfish automatically suggests the most recently available downloaded version of the requested model. This reduces the time-to-first-inference, but may not be desirable for your use case. Download the model before starting your service if you need the most recent version available on Hugging Face.</p> <p>Tip</p> <p>Add the <code>--dry-run</code> flag to preview the start-up script that Blackfish will submit.</p>"},{"location":"getting_started/#ls-list-services","title":"<code>ls</code> - List services","text":"<p>To view a list of your Blackfish services, type <pre><code>blackfish ls # --filter id=&lt;service_id&gt;,status=&lt;status&gt;\nSERVICE ID      IMAGE                MODEL                                CREATED       UPDATED     STATUS    PORT   NAME              PROFILE\n97ffde37-7e02   speech_recognition   openai/whisper-large-v3              7 hours ago   1 min ago   HEALTHY   8082   blackfish-11846   default\nfed36739-70b4   text_generation      TinyLlama/TinyLlama-1.1B-Chat-v1.0   7 sec ago     5 sec ago   PENDING   None   blackfish-89359   della\n</code></pre> The last item in this list is the service we just started. In this case, the <code>default</code> profile happens to be set up to connect to a remote HPC cluster, so the service is run as a Slurm job. It may take a few minutes for our Slurm job to start, and it will require additional time for the service to be ready after that. Until then, our service's status will be either <code>SUBMITTED</code>, <code>PENDING</code> or <code>STARTING</code>. Now would be a good time to brew a hot beverage \u2615\ufe0f.</p> <p>Tip</p> <p>If you ever want more detailed information about a service, you can get it with the <code>blackfish details &lt;service_id&gt;</code> command. Again, <code>--help</code> is your friend if you want more information.</p> <p>Now that we're refreshed, let's see how our service is doing. Re-run the command above. If things went smoothly, then we should see that the service's status has changed to <code>HEALTHY</code> (if your service is still <code>STARTING</code>, give it another minute and try again).</p> <pre><code>blackfish ls\nSERVICE ID      IMAGE                MODEL                                CREATED       UPDATED      STATUS    PORT   NAME              PROFILE\n97ffde37-7e02   speech_recognition   openai/whisper-large-v3              7 hours ago   19 sec ago   HEALTHY   8082   blackfish-11846   default\nfed36739-70b4   text_generation      TinyLlama/TinyLlama-1.1B-Chat-v1.0   2 min ago     19 sec ago   HEALTHY   8080   blackfish-12328   della\n</code></pre> <p>At this point, we can start interacting with the service. Let's say \"Hello\", shall we?</p> <p>The details of calling a service depend on the service you are trying to connect to. For the <code>text-generation</code> service, the primary endpoint is accessed like so:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer sealsaretasty\" \\\n  -d '{\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are an expert marine biologist.\"},\n            {\"role\": \"user\", \"content\": \"Why are orcas so awesome?\"}\n        ],\n        \"max_completion_tokens\": 100,\n        \"temperature\": 0.1,\n        \"stream\": false\n    }' | jq\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1192  100   911  100   281   1652    509 --:--:-- --:--:-- --:--:--  2159\n{\n  \"id\": \"chatcmpl-b6452981728f4f3cb563960d6639f8a4\",\n  \"object\": \"chat.completion\",\n  \"created\": 1747826716,\n  \"model\": \"/data/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"reasoning_content\": null,\n        \"content\": \"Orcas (also known as killer whales) are incredibly intelligent and social animals that are known for their incredible abilities. Here are some reasons why orcas are so awesome:\\n\\n1. Intelligence: Orcas are highly intelligent and have been observed using tools, communicating with each other, and even learning from their trainers.\\n\\n2. Social behavior: Orcas are highly social animals and form complex social structures, including family groups, pods,\",\n        \"tool_calls\": []\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 40,\n    \"total_tokens\": 140,\n    \"completion_tokens\": 100,\n    \"prompt_tokens_details\": null\n  },\n  \"prompt_logprobs\": null\n}\n</code></pre> <p>Most services provide a single endpoint that performs a task or pipeline. For text generation, there two main endpoints, <code>/v1/completions</code> and <code>/v1/chat/completions</code>. Running services are yours to use as you see fit.</p>"},{"location":"getting_started/#stop-stop-a-service","title":"<code>stop</code> - Stop a service","text":"<p>When we are done with our service, we should shut it off and return its resources to the cluster. To do so, simply type <pre><code>blackfish stop fed36739-70b4-4dc4-8017-a4277563aef9\n\u2714 Stopped service fed36739-70b4-4dc4-8017-a4277563aef9\n</code></pre> You should receive a nice message stating that the service was stopped, which you can confirm by checking its status with <code>blackfish ls</code>.</p>"},{"location":"getting_started/#rm-delete-a-service","title":"<code>rm</code> - Delete a service","text":"<p>Services aren't automatically deleted from your list, so it's a good idea to remove them when you're done if you don't need them for record keeping: <pre><code>blackfish rm --filters id=fed36739-70b4-4dc4-8017-a4277563aef9\n\u2714 Removed 1 service.\n</code></pre></p>"},{"location":"getting_started/#speech-recognition","title":"Speech Recognition","text":"<pre><code>blackfish run --profile default speech-recognition openai/whisper-large-v3\n\u2714 Found 4 models.\n\u2714 Found 1 snapshots.\n\u26a0 No revision provided. Using latest available commit: 06f233fe06e710322aca913c1bc4249a0d71fce1.\n\u2714 Found model openai/whisper-large-v3!\n\u2714 Started service: 70e59004-84d4-4f7c-bf78-95ef96054289\n</code></pre> <pre><code>curl http://localhost:8080/transcribe \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"audio_path\": \"/data/audio/NY045-0.mp3\",\n        \"response_format\": \"text\"\n    }' | jq\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   465  100   375  100    90     10      2  0:00:45  0:00:35  0:00:10    91\n{\n  \"audio_path\": \"/data/audio/NY045-0.mp3\",\n  \"text\": \" Oh, going to Cuba. I went to Cuba on Prohibition time, too. And I brought a lot of fancy bottles, a little, like one drink in for souvenirs for all my friends. Well, Atlantic City, the ship was stopped. It was all in the newspapers about it, too. The crew had ripped the walls and put all\",\n  \"segments\": null,\n  \"task\": \"transcribe\"\n}\n</code></pre> <ol> <li> <p>The list of models you see depends on your environment. If you do not have access to a shared HPC cache, your list of models is likely empty. Not to worry\u2014we will see how to add models later on.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/jobs/","title":"API Documentation","text":""},{"location":"api/jobs/#app.job","title":"<code>app.job</code>","text":""},{"location":"api/jobs/#app.job.LocalJob","title":"<code>LocalJob</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Job</code></p> <p>A light-weight local job dataclass.</p>"},{"location":"api/jobs/#app.job.LocalJobConfig","title":"<code>LocalJobConfig</code>  <code>dataclass</code>","text":"<p>Job configuration for running a service locally.</p>"},{"location":"api/jobs/#app.job.SlurmJob","title":"<code>SlurmJob</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Job</code></p> <p>A light-weight Slurm job dataclass.</p>"},{"location":"api/jobs/#app.job.SlurmJob.cancel","title":"<code>cancel() -&gt; None</code>","text":"<p>Cancel a Slurm job by issuing the <code>scancel</code> command on the remote host.</p> <p>This method logs a warning if the update fails, but does not raise an exception.</p>"},{"location":"api/jobs/#app.job.SlurmJob.fetch_node","title":"<code>fetch_node() -&gt; Optional[str]</code>","text":"<p>Attempt to update the job node from Slurm accounting and return the new node (or the current node if the update fails).</p> <p>This method logs a warning if the update fails, but does not raise an exception.</p>"},{"location":"api/jobs/#app.job.SlurmJob.fetch_port","title":"<code>fetch_port() -&gt; Optional[int]</code>","text":"<p>Attempt to update the job port and return the new port (or the current port if the update fails)</p> <p>The job port is stored as a directory in the remote Blackfish home when a port is assigned to a service container.</p> <p>This method logs a warning if the update fails, but does not raise an exception.</p>"},{"location":"api/jobs/#app.job.SlurmJob.update","title":"<code>update(verbose: bool = False) -&gt; Optional[str]</code>","text":"<p>Attempt to update the job state from Slurm accounting and return the new state (or current state if the update fails).</p> <p>If the job state switches from PENDING or MISSING to RUNNING, also update the job node and port.</p> <p>This method logs a warning if the update fails, but does not raise an exception.</p>"},{"location":"api/jobs/#app.job.SlurmJob.wait","title":"<code>wait(period: int = 5) -&gt; dict[str, bool]</code>","text":"<p>Wait for the job to start, re-checking the job's status every <code>period</code> seconds.</p>"},{"location":"api/jobs/#app.job.SlurmJobConfig","title":"<code>SlurmJobConfig</code>  <code>dataclass</code>","text":"<p>Job configuration for running a service as a Slurm job.</p>"},{"location":"api/services/","title":"Services","text":""},{"location":"api/services/#app.services.base","title":"<code>app.services.base</code>","text":""},{"location":"api/services/#app.services.base.Service","title":"<code>Service</code>","text":"<p>               Bases: <code>UUIDAuditBase</code></p>"},{"location":"api/services/#app.services.base.Service.close_tunnel","title":"<code>close_tunnel(session: AsyncSession) -&gt; None</code>  <code>async</code>","text":"<p>Kill the ssh tunnel connecting to the API. Assumes attached to session.</p> <p>Finds all processes named \"ssh\" and kills any associated with the service's local port.</p> <p>This is equivalent to the shell command:</p> <pre><code>pid = $(ps aux | grep ssh | grep 8080\")\nkill $pid\n</code></pre>"},{"location":"api/services/#app.services.base.Service.get_job","title":"<code>get_job(verbose: bool = False) -&gt; Job | None</code>","text":"<p>Fetch the job backing the service.</p>"},{"location":"api/services/#app.services.base.Service.open_tunnel","title":"<code>open_tunnel(job: SlurmJob) -&gt; None</code>  <code>async</code>","text":"<p>Create an ssh tunnel to connect to the service. Assumes attached to session.</p> <p>After creation of the tunnel, the remote port is updated and recorded in the database.</p>"},{"location":"api/services/#app.services.base.Service.refresh","title":"<code>refresh(session: AsyncSession, app_config: State) -&gt; Optional[ServiceStatus]</code>  <code>async</code>","text":"<p>Update the service status. Assumes running in an attached state.</p> <p>Determines the service status by pinging the service and then checking the Slurm job state if the ping in unsuccessful. Updates the service database and returns the status.</p> <p>The status returned depends on the starting status because services in a \"STARTING\" status cannot transitionto an \"UNHEALTHY\" status. The status life-cycle is as follows:</p> <pre><code>Slurm job submitted -&gt; SUBMITTED\n    Slurm job switches to pending -&gt; PENDING\n        Slurm job switches to running -&gt; STARTING\n            API ping successful -&gt; HEALTHY\n            API ping unsuccessful -&gt; STARTING\n            API ping unsuccessful and time limit exceeded -&gt; TIMEOUT\n        Slurm job switches to failed -&gt; FAILED\n    Slurm job switches to failed -&gt; FAILED\n</code></pre> <p>A service that successfully starts will be in a HEALTHY status. The status remains HEALTHY as long as subsequent updates ping successfully. Unsuccessful pings will transition the service status to FAILED if the Slurm job has failed; TIMEOUT if the Slurm job times out; and UNHEALTHY otherwise.</p> <p>An UNHEALTHY service becomes HEALTHY if the update pings successfully. Otherwise, the service status changes to FAILED if the Slurm job has failed or TIMEOUT if the Slurm job times out.</p> <p>Services that enter a terminal status (FAILED, TIMEOUT or STOPPED) cannot be re-started.</p>"},{"location":"api/services/#app.services.base.Service.start","title":"<code>start(session: AsyncSession, app_config: State, container_options: BaseConfig, job_options: JobConfig) -&gt; None</code>  <code>async</code>","text":"<p>Start the service with provided Slurm job and container options. Assumes running in attached state.</p> <p>Submits a Slurm job request, creates a new database entry and waits for the service to start.</p> <p>Parameters:</p> <ul> <li> <code>container_options</code>               (<code>BaseConfig</code>)           \u2013            <p>a dict containing container options (see ContainerConfig).</p> </li> <li> <code>job_options</code>               (<code>JobConfig</code>)           \u2013            <p>a dict containing job options (see JobConfig).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None.</p> </li> </ul>"},{"location":"api/services/#app.services.base.Service.stop","title":"<code>stop(session: AsyncSession, timeout: bool = False, failed: bool = False) -&gt; None</code>  <code>async</code>","text":"<p>Stop the service. Assumes running in attached state.</p> <p>The default terminal state is STOPPED, which indicates that the service was stopped normally. Use the <code>failed</code> or <code>timeout</code> flags to indicate that the service stopped due to a Slurm job failure or timeout, resp.</p> <p>This process updates the database after stopping the service.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>flag indicating the service timed out.</p> </li> <li> <code>failed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>flag indicating the service Slurm job failed.</p> </li> </ul>"},{"location":"api/services/#app.services.text_generation","title":"<code>app.services.text_generation</code>","text":""},{"location":"api/services/#app.services.text_generation.TextGeneration","title":"<code>TextGeneration</code>","text":"<p>               Bases: <code>Service</code></p> <p>A containerized service running a text-generation API.</p>"},{"location":"api/services/#app.services.text_generation.TextGenerationParameters","title":"<code>TextGenerationParameters</code>  <code>dataclass</code>","text":"<p>See https://platform.openai.com/docs/api-reference/chat and https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#chat-api.</p>"},{"location":"api/swagger/","title":"OpenAPI (Swagger)","text":""}]}