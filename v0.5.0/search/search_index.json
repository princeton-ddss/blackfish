{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#_1","title":"Welcome","text":"<p> Welcome to Blackfish! Blackfish is an open source \"ML-as-a-Service\" (MLaaS) platform that helps researchers use state-of-the-art, open source artificial intelligence and machine learning models. With Blackfish, researchers can spin up their own version of popular public cloud services (e.g., ChatGPT, Amazon Transcribe, etc.) using high-performance computing (HPC) resources already available on campus. </p> <p>The primary goal of Blackfish is to facilitate transparent and reproducible research based on open source machine learning and artificial intelligence. We do this by providing mechanisms to run user-specified models with user-defined configurations. For academic research, open source models present several advantages over closed source models. First, whereas large-scale projects using public cloud services might cost $10K to $100K for similar quality results, open source models running on HPC resources are free to researchers. Second, with open source models you know exactly what model you are using and you can easily provide a copy of that model to other researchers. Closed source models can and do change without notice. Third, using open-source models allows complete transparency into how your data is being used.</p>"},{"location":"#why-should-you-use-blackfish","title":"Why should you use Blackfish?","text":""},{"location":"#1-its-easy","title":"1. It's easy! \ud83c\udf08","text":"<p>Researchers should focus on research, not tooling. We try to meet researchers where they're at by providing multiple ways to work with Blackfish, including a Python API, a command-line tool (CLI), and a browser-based user interface (UI).</p> <p>Don't want to install a Python package? Ask your HPC admins to install Blackfish OnDemand!</p>"},{"location":"#2-its-transparent","title":"2. It's transparent \ud83e\uddd0","text":"<p>You decide what model to run (down to the Git commit) and how you want it configured. There are no unexpected (or undetected) changes in performance because the model is always the same. All services are private, so you know exactly how your data is being handled.</p>"},{"location":"#3-its-free","title":"3. It's free! \ud83d\udcb8","text":"<p>You have an HPC cluster. We have software to run on it.</p>"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#python","title":"Python","text":"<p>Blackfish requires Python to run locally. Alternatively, Blackfish can be added to your university's Open OnDemand portal, which allows users to run applications on HPC resources through a web browser. For more information, see our companion repo blackfish-ondemand.</p>"},{"location":"#docker-apptainer","title":"Docker &amp; Apptainer","text":"<p>Blackfish uses Docker or Apptainer to run service containers locally. HPC-based services require Apptainer to be installed on your university cluster.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#step-1-install-blackfish","title":"Step 1 - Install blackfish","text":"<pre><code>pip install blackfish-ai\n</code></pre>"},{"location":"#step-2-create-a-profile","title":"Step 2 - Create a profile","text":"<pre><code>blackfish init\n\n# Example responses\n# &gt; name: default\n# &gt; type: slurm\n# &gt; host: cluster.organization.edu\n# &gt; user: shamu\n# &gt; home: /home/shamu/.blackfish\n# &gt; cache: /shared/.blackfish\n</code></pre>"},{"location":"#step-3-start-the-api","title":"Step 3 - Start the API","text":"<pre><code>blackfish start\n</code></pre>"},{"location":"#step-4-obtain-a-model","title":"Step 4 - Obtain a model","text":"<pre><code>blackfish model add --profile default openai/whisper-large-v3  # This will take a minute...\n</code></pre>"},{"location":"#step-5-run-a-service","title":"Step 5 - Run a service","text":"<pre><code>blackfish run --mount $HOME/Downloads speech-recognition openai/whisper-large-v3\n</code></pre>"},{"location":"#step-6-submit-a-request","title":"Step 6 - Submit a request","text":"<pre><code># First, check the service status...\nblackfish ls\n\n# Once the service is healthy...\ncurl -X POST 'http://localhost:8080/transcribe' -H 'Content-Type: application/json' -d '{\"audio_path\": \"/data/audio/NY045.mp3\", \"response_format\": \"json\"}'\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Ready to give Blackfish a try? Check out our setup guide.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Blackfish is maintained by research software engineers at Princeton University's Data Driven Social Science Initiative.</p>"},{"location":"api/core/","title":"API Reference","text":""},{"location":"api/core/#blackfish.client","title":"<code>blackfish.client</code>","text":"<p>Blackfish client for the programmatic interface.</p>"},{"location":"api/core/#blackfish.client.Blackfish","title":"<code>Blackfish</code>","text":"<p>Programmatic interface for managing Blackfish ML inference services.</p> <p>This client provides both synchronous and asynchronous APIs for creating, managing, and monitoring ML inference services. All async methods are prefixed with 'async_' (e.g., async_launch_service, async_list_services).</p> <p>Examples:</p> <p>Synchronous usage: <pre><code>&gt;&gt;&gt; bf = Blackfish()\n&gt;&gt;&gt; service = bf.launch_service(\n...     name=\"my-llm\",\n...     image=\"text_generation\",\n...     model=\"meta-llama/Llama-3.3-70B-Instruct\",\n...     profile_name=\"default\"\n... )\n&gt;&gt;&gt; print(service.status)\n</code></pre></p> <p>Asynchronous usage: <pre><code>&gt;&gt;&gt; async def main():\n...     bf = Blackfish()\n...     service = await bf.async_launch_service(\n...         name=\"my-llm\",\n...         image=\"text_generation\",\n...         model=\"meta-llama/Llama-3.3-70B-Instruct\",\n...         profile_name=\"default\"\n...     )\n...     print(service.status)\n&gt;&gt;&gt; asyncio.run(main())\n</code></pre></p>"},{"location":"api/core/#blackfish.client.Blackfish.home_dir","title":"<code>home_dir: str</code>  <code>property</code>","text":"<p>Get the Blackfish home directory.</p>"},{"location":"api/core/#blackfish.client.Blackfish.__aenter__","title":"<code>__aenter__() -&gt; Self</code>  <code>async</code>","text":"<p>Async context manager entry.</p>"},{"location":"api/core/#blackfish.client.Blackfish.__aexit__","title":"<code>__aexit__(exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: Any) -&gt; None</code>  <code>async</code>","text":"<p>Async context manager exit.</p>"},{"location":"api/core/#blackfish.client.Blackfish.__enter__","title":"<code>__enter__() -&gt; Self</code>","text":"<p>Sync context manager entry.</p>"},{"location":"api/core/#blackfish.client.Blackfish.__exit__","title":"<code>__exit__(exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: Any) -&gt; None</code>","text":"<p>Sync context manager exit.</p>"},{"location":"api/core/#blackfish.client.Blackfish.__init__","title":"<code>__init__(home_dir: Optional[str] = None, host: Optional[str] = None, port: Optional[int] = None, debug: Optional[bool] = None, auth_token: Optional[str] = None, config: Optional[BlackfishConfig] = None)</code>","text":"<p>Initialize the Blackfish client.</p> <p>You can either pass a complete BlackfishConfig object, or pass individual configuration parameters. Individual parameters will override values from a provided config object.</p> <p>Parameters:</p> <ul> <li> <code>home_dir</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Path to Blackfish home directory (default: ~/.blackfish)</p> </li> <li> <code>host</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>API host (default: localhost)</p> </li> <li> <code>port</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>API port (default: 8000)</p> </li> <li> <code>debug</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Debug mode (default: True)</p> </li> <li> <code>auth_token</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Authentication token (optional)</p> </li> <li> <code>config</code>               (<code>Optional[BlackfishConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Optional BlackfishConfig instance for advanced configuration.    Individual parameters will override config values if provided.</p> </li> </ul> <p>Examples:</p> <p>Simple usage: <pre><code>&gt;&gt;&gt; bf = Blackfish(home_dir=\"~/.blackfish\", debug=True)\n</code></pre></p> <p>Advanced usage with full config: <pre><code>&gt;&gt;&gt; config = BlackfishConfig(home_dir=\"~/.blackfish\", port=9000)\n&gt;&gt;&gt; bf = Blackfish(config=config)\n</code></pre></p> <p>Mixed usage (config + overrides): <pre><code>&gt;&gt;&gt; config = BlackfishConfig(...)\n&gt;&gt;&gt; bf = Blackfish(config=config, port=9000)  # Override just the port\n</code></pre></p>"},{"location":"api/core/#blackfish.client.Blackfish.async_delete_service","title":"<code>async_delete_service(service_id: str) -&gt; bool</code>  <code>async</code>","text":"<p>Delete a service from the database (async).</p> <p>Note: This only deletes the database record. The service should be stopped first using stop_service().</p> <p>Parameters:</p> <ul> <li> <code>service_id</code>               (<code>str</code>)           \u2013            <p>UUID of the service</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if deleted, False if not found</p> </li> </ul>"},{"location":"api/core/#blackfish.client.Blackfish.async_get_service","title":"<code>async_get_service(service_id: str) -&gt; Optional[ManagedService]</code>  <code>async</code>","text":"<p>Get a service by ID (async).</p> <p>Parameters:</p> <ul> <li> <code>service_id</code>               (<code>str</code>)           \u2013            <p>UUID of the service</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ManagedService]</code>           \u2013            <p>ManagedService instance or None if not found</p> </li> </ul>"},{"location":"api/core/#blackfish.client.Blackfish.async_launch_service","title":"<code>async_launch_service(name: str, image: str, model: str, profile_name: str = 'default', container_config: Optional[dict[str, Any]] = None, job_config: Optional[dict[str, Any]] = None, mount: Optional[str] = None, grace_period: int = 180, auto_cleanup: bool = True, **kwargs: dict[str, Any]) -&gt; ManagedService</code>  <code>async</code>","text":"<p>Create and start a new service (async).</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Service name</p> </li> <li> <code>image</code>               (<code>str</code>)           \u2013            <p>Service image type (e.g., \"text_generation\", \"speech_recognition\")</p> </li> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Model repository ID (e.g., \"meta-llama/Llama-3.3-70B-Instruct\")</p> </li> <li> <code>profile_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>Name of the profile to use</p> </li> <li> <code>container_config</code>               (<code>Optional[dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Container configuration options. If 'model_dir' and 'revision' are not provided, they will be automatically determined by searching for the model in the profile's cache directories and selecting the latest revision.</p> </li> <li> <code>job_config</code>               (<code>Optional[dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Job configuration options (Slurm settings, etc.)</p> </li> <li> <code>mount</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional directory to mount</p> </li> <li> <code>grace_period</code>               (<code>int</code>, default:                   <code>180</code> )           \u2013            <p>Time in seconds to wait before marking unhealthy</p> </li> <li> <code>auto_cleanup</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, automatically stop and delete this service when the Python script exits (default: True)</p> </li> <li> <code>**kwargs</code>               (<code>dict[str, Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional service-specific parameters</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ManagedService</code> (              <code>ManagedService</code> )          \u2013            <p>The created service instance wrapped for easy access</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the profile is not found, the model is not available, or model files cannot be located.</p> </li> </ul>"},{"location":"api/core/#blackfish.client.Blackfish.async_list_services","title":"<code>async_list_services(image: Optional[str] = None, model: Optional[str] = None, status: Optional[ServiceStatus] = None, name: Optional[str] = None, profile: Optional[str] = None) -&gt; list[ManagedService]</code>  <code>async</code>","text":"<p>List services with optional filters (async).</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Filter by image type</p> </li> <li> <code>model</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Filter by model</p> </li> <li> <code>status</code>               (<code>Optional[ServiceStatus]</code>, default:                   <code>None</code> )           \u2013            <p>Filter by status</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Filter by name</p> </li> <li> <code>profile</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Filter by profile</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[ManagedService]</code>           \u2013            <p>List of matching managed services</p> </li> </ul>"},{"location":"api/core/#blackfish.client.Blackfish.async_stop_service","title":"<code>async_stop_service(service_id: str, timeout: bool = False, failed: bool = False) -&gt; Optional[ManagedService]</code>  <code>async</code>","text":"<p>Stop a service (async).</p> <p>Parameters:</p> <ul> <li> <code>service_id</code>               (<code>str</code>)           \u2013            <p>UUID of the service</p> </li> <li> <code>timeout</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Mark as timed out</p> </li> <li> <code>failed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Mark as failed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ManagedService]</code>           \u2013            <p>Updated managed service instance or None if not found</p> </li> </ul>"},{"location":"api/core/#blackfish.client.Blackfish.async_wait_for_service","title":"<code>async_wait_for_service(service_id: str, target_status: ServiceStatus = ServiceStatus.HEALTHY, timeout: float = 300, poll_interval: float = 5) -&gt; Optional[ManagedService]</code>  <code>async</code>","text":"<p>Wait for a service to reach a target status (async).</p> <p>Parameters:</p> <ul> <li> <code>service_id</code>               (<code>str</code>)           \u2013            <p>UUID of the service</p> </li> <li> <code>target_status</code>               (<code>ServiceStatus</code>, default:                   <code>HEALTHY</code> )           \u2013            <p>Status to wait for (default: HEALTHY)</p> </li> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>300</code> )           \u2013            <p>Maximum time to wait in seconds (default: 300)</p> </li> <li> <code>poll_interval</code>               (<code>float</code>, default:                   <code>5</code> )           \u2013            <p>Time between status checks in seconds (default: 5)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ManagedService]</code>           \u2013            <p>ManagedService instance if target status reached, None if timeout or service failed</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; service = bf.launch_service(...)\n&gt;&gt;&gt; service = await bf.async_wait_for_service(str(service.id))\n&gt;&gt;&gt; if service and service.status == ServiceStatus.HEALTHY:\n...     print(f\"Service ready on port {service.port}\")\n</code></pre>"},{"location":"api/core/#blackfish.client.Blackfish.close","title":"<code>close() -&gt; None</code>","text":"<p>Close the database connection (sync wrapper).</p>"},{"location":"api/core/#blackfish.client.Blackfish.delete_service","title":"<code>delete_service(service_id: str) -&gt; bool</code>  <code>async</code>","text":"<p>Delete a service from the database (sync wrapper).</p> <p>See async_delete_service for details.</p>"},{"location":"api/core/#blackfish.client.Blackfish.get_service","title":"<code>get_service(service_id: str) -&gt; Optional[ManagedService]</code>  <code>async</code>","text":"<p>Get a service by ID (sync wrapper).</p> <p>See async_get_service for details.</p>"},{"location":"api/core/#blackfish.client.Blackfish.launch_service","title":"<code>launch_service(name: str, image: str, model: str, profile_name: str = 'default', container_config: Optional[dict[str, Any]] = None, job_config: Optional[dict[str, Any]] = None, mount: Optional[str] = None, grace_period: int = 180, auto_cleanup: bool = True, **kwargs: dict[str, Any]) -&gt; ManagedService</code>  <code>async</code>","text":"<p>Create and start a new service (sync wrapper).</p> <p>See async_launch_service for details.</p>"},{"location":"api/core/#blackfish.client.Blackfish.list_services","title":"<code>list_services(image: Optional[str] = None, model: Optional[str] = None, status: Optional[ServiceStatus] = None, name: Optional[str] = None, profile: Optional[str] = None) -&gt; list[ManagedService]</code>  <code>async</code>","text":"<p>List services with optional filters (sync wrapper).</p> <p>See async_list_services for details.</p>"},{"location":"api/core/#blackfish.client.Blackfish.stop_service","title":"<code>stop_service(service_id: str, timeout: bool = False, failed: bool = False) -&gt; Optional[ManagedService]</code>  <code>async</code>","text":"<p>Stop a service (sync wrapper).</p> <p>See async_stop_service for details.</p>"},{"location":"api/core/#blackfish.client.Blackfish.wait_for_service","title":"<code>wait_for_service(service_id: str, target_status: ServiceStatus = ServiceStatus.HEALTHY, timeout: float = 300, poll_interval: float = 5) -&gt; Optional[ManagedService]</code>  <code>async</code>","text":"<p>Wait for a service to reach a target status (sync wrapper).</p> <p>See async_wait_for_service for details.</p>"},{"location":"api/core/#blackfish.service","title":"<code>blackfish.service</code>","text":"<p>ManagedService wrapper class for the Blackfish programmatic interface.</p>"},{"location":"api/core/#blackfish.service.ManagedService","title":"<code>ManagedService</code>","text":"<p>Wrapper around Service that provides convenient access to service methods.</p> <p>This class wraps a Service object and provides easy-to-use methods that don't require passing session and state objects. All operations are delegated to the parent Blackfish client.</p>"},{"location":"api/core/#blackfish.service.ManagedService.__getattr__","title":"<code>__getattr__(name: str) -&gt; Any</code>","text":"<p>Delegate attribute access to the underlying service.</p>"},{"location":"api/core/#blackfish.service.ManagedService.__init__","title":"<code>__init__(service: Service, client: Blackfish)</code>","text":"<p>Initialize a managed service.</p> <p>Parameters:</p> <ul> <li> <code>service</code>               (<code>Service</code>)           \u2013            <p>The underlying Service object</p> </li> <li> <code>client</code>               (<code>Blackfish</code>)           \u2013            <p>The Blackfish client managing this service</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Return string representation.</p>"},{"location":"api/core/#blackfish.service.ManagedService.async_close_tunnel","title":"<code>async_close_tunnel() -&gt; Self</code>  <code>async</code>","text":"<p>Close the SSH tunnel for this service (async).</p> <p>This is useful when a service didn't properly release its port. Finds and kills SSH processes associated with the service's port.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Self for method chaining</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.async_delete","title":"<code>async_delete() -&gt; bool</code>  <code>async</code>","text":"<p>Delete the service from the database (async).</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if deleted successfully</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.async_refresh","title":"<code>async_refresh() -&gt; Self</code>  <code>async</code>","text":"<p>Refresh the service status (async).</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Self for method chaining</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.async_stop","title":"<code>async_stop(timeout: bool = False, failed: bool = False) -&gt; Self</code>  <code>async</code>","text":"<p>Stop the service (async).</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Mark as timed out</p> </li> <li> <code>failed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Mark as failed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Self for method chaining</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.async_wait","title":"<code>async_wait(timeout: float = 300, poll_interval: float = 10) -&gt; Self</code>  <code>async</code>","text":"<p>Wait for the service to be healthy.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>300</code> )           \u2013            <p>Maximum time to wait in seconds (default: 300)</p> </li> <li> <code>poll_interval</code>               (<code>float</code>, default:                   <code>10</code> )           \u2013            <p>Time between status checks in seconds (default: 10)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Self (for method chaining), or None if service not found</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; service = await bf.async_launch_service(...)\n&gt;&gt;&gt; service = await service.async_wait()\n&gt;&gt;&gt; if service and service.status == ServiceStatus.HEALTHY:\n...     print(f\"Service ready on port {service.port}\")\n</code></pre>"},{"location":"api/core/#blackfish.service.ManagedService.close_tunnel","title":"<code>close_tunnel() -&gt; Self</code>","text":"<p>Close the SSH tunnel for this service (sync).</p> <p>This is useful when a service didn't properly release its port. Finds and kills SSH processes associated with the service's port.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Self for method chaining</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.delete","title":"<code>delete() -&gt; bool</code>","text":"<p>Delete the service from the database (sync).</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if deleted successfully</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.refresh","title":"<code>refresh() -&gt; Self</code>","text":"<p>Refresh the service status (sync).</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Self for method chaining</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.stop","title":"<code>stop(timeout: bool = False, failed: bool = False) -&gt; Self</code>","text":"<p>Stop the service (sync).</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Mark as timed out</p> </li> <li> <code>failed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Mark as failed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Self for method chaining</p> </li> </ul>"},{"location":"api/core/#blackfish.service.ManagedService.wait","title":"<code>wait(timeout: float = 300, poll_interval: float = 10) -&gt; Self</code>","text":"<p>Wait for the service to be healthy (sync).</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>300</code> )           \u2013            <p>Maximum time to wait in seconds (default: 300)</p> </li> <li> <code>poll_interval</code>               (<code>float</code>, default:                   <code>10</code> )           \u2013            <p>Time between status checks in seconds (default: 10)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Self (for method chaining), or None if service not found</p> </li> </ul>"},{"location":"api/core/#blackfish.utils","title":"<code>blackfish.utils</code>","text":"<p>Utility functions for the Blackfish programmatic interface.</p>"},{"location":"api/core/#blackfish.utils.set_logging_level","title":"<code>set_logging_level(level: str = 'WARNING') -&gt; None</code>","text":"<p>Set the global Blackfish logging level.</p> <p>This function controls the logging level for all Blackfish operations. By default, the programmatic interface uses WARNING level to reduce verbose output. Use this function to change the logging level globally.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>str</code>, default:                   <code>'WARNING'</code> )           \u2013            <p>Logging level string. Must be one of: \"DEBUG\", \"INFO\", \"WARNING\",    \"ERROR\", or \"CRITICAL\". Case-insensitive.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import blackfish\n&gt;&gt;&gt; blackfish.set_logging_level(\"INFO\")  # Show info logs\n&gt;&gt;&gt; blackfish.set_logging_level(\"DEBUG\")  # Show all logs including debug\n&gt;&gt;&gt; blackfish.set_logging_level(\"WARNING\")  # Only warnings and errors (default)\n</code></pre>"},{"location":"api/jobs/","title":"Jobs","text":""},{"location":"api/jobs/#blackfish.server.job","title":"<code>blackfish.server.job</code>","text":""},{"location":"api/jobs/#blackfish.server.job.Job","title":"<code>Job</code>  <code>dataclass</code>","text":"<p>Abstract job class.</p>"},{"location":"api/jobs/#blackfish.server.job.LocalJob","title":"<code>LocalJob</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Job</code></p> <p>A light-weight local job dataclass.</p>"},{"location":"api/jobs/#blackfish.server.job.LocalJobConfig","title":"<code>LocalJobConfig</code>  <code>dataclass</code>","text":"<p>Job configuration for running a service locally.</p>"},{"location":"api/jobs/#blackfish.server.job.SlurmJob","title":"<code>SlurmJob</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Job</code></p> <p>A light-weight Slurm job dataclass.</p>"},{"location":"api/jobs/#blackfish.server.job.SlurmJob.cancel","title":"<code>cancel() -&gt; None</code>","text":"<p>Cancel a Slurm job by issuing the <code>scancel</code> command on the remote host.</p> <p>This method logs a warning if the update fails, but does not raise an exception.</p>"},{"location":"api/jobs/#blackfish.server.job.SlurmJob.fetch_node","title":"<code>fetch_node() -&gt; Optional[str]</code>","text":"<p>Attempt to update the job node from Slurm accounting and return the new node (or the current node if the update fails).</p> <p>This method logs a warning if the update fails, but does not raise an exception.</p>"},{"location":"api/jobs/#blackfish.server.job.SlurmJob.fetch_port","title":"<code>fetch_port() -&gt; Optional[int]</code>","text":"<p>Attempt to update the job port and return the new port (or the current port if the update fails)</p> <p>The job port is stored as a directory in the remote Blackfish home when a port is assigned to a service container.</p> <p>This method logs a warning if the update fails, but does not raise an exception.</p>"},{"location":"api/jobs/#blackfish.server.job.SlurmJob.update","title":"<code>update(verbose: bool = False) -&gt; Optional[str]</code>","text":"<p>Attempt to update the job state from Slurm accounting and return the new state (or current state if the update fails).</p> <p>If the job state switches from PENDING or MISSING to RUNNING, also update the job node and port.</p> <p>This method logs a warning if the update fails, but does not raise an exception.</p>"},{"location":"api/jobs/#blackfish.server.job.SlurmJob.wait","title":"<code>wait(period: int = 5) -&gt; dict[str, bool]</code>","text":"<p>Wait for the job to start, re-checking the job's status every <code>period</code> seconds.</p>"},{"location":"api/jobs/#blackfish.server.job.SlurmJobConfig","title":"<code>SlurmJobConfig</code>  <code>dataclass</code>","text":"<p>Job configuration for running a service as a Slurm job.</p>"},{"location":"api/jobs/#blackfish.server.job.format_state","title":"<code>format_state(status: JobState | str | None) -&gt; str</code>","text":"<p>Format job status for display.</p>"},{"location":"api/services/","title":"Services","text":""},{"location":"api/services/#blackfish.server.services.base","title":"<code>blackfish.server.services.base</code>","text":""},{"location":"api/services/#blackfish.server.services.base.Service","title":"<code>Service</code>","text":"<p>               Bases: <code>UUIDAuditBase</code></p>"},{"location":"api/services/#blackfish.server.services.base.Service.close_tunnel","title":"<code>close_tunnel(session: AsyncSession) -&gt; None</code>  <code>async</code>","text":"<p>Kill the ssh tunnel connecting to the API. Assumes attached to session.</p> <p>Finds all processes named \"ssh\" and kills any associated with the service's local port.</p> <p>This is equivalent to the shell command:</p> <pre><code>pid = $(ps aux | grep ssh | grep 8080\")\nkill $pid\n</code></pre>"},{"location":"api/services/#blackfish.server.services.base.Service.get_job","title":"<code>get_job(verbose: bool = False) -&gt; Job | None</code>","text":"<p>Fetch the job backing the service.</p>"},{"location":"api/services/#blackfish.server.services.base.Service.open_tunnel","title":"<code>open_tunnel(job: SlurmJob) -&gt; None</code>  <code>async</code>","text":"<p>Create an ssh tunnel to connect to the service. Assumes attached to session.</p> <p>After creation of the tunnel, the remote port is updated and recorded in the database.</p>"},{"location":"api/services/#blackfish.server.services.base.Service.refresh","title":"<code>refresh(session: AsyncSession, app_config: State) -&gt; Optional[ServiceStatus]</code>  <code>async</code>","text":"<p>Update the service status. Assumes running in an attached state.</p> <p>Determines the service status by pinging the service and then checking the Slurm job state if the ping in unsuccessful. Updates the service database and returns the status.</p> <p>The status returned depends on the starting status because services in a \"STARTING\" status cannot transitionto an \"UNHEALTHY\" status. The status life-cycle is as follows:</p> <pre><code>Slurm job submitted -&gt; SUBMITTED\n    Slurm job switches to pending -&gt; PENDING\n        Slurm job switches to running -&gt; STARTING\n            API ping successful -&gt; HEALTHY\n            API ping unsuccessful -&gt; STARTING\n            API ping unsuccessful and time limit exceeded -&gt; TIMEOUT\n        Slurm job switches to failed -&gt; FAILED\n    Slurm job switches to failed -&gt; FAILED\n</code></pre> <p>A service that successfully starts will be in a HEALTHY status. The status remains HEALTHY as long as subsequent updates ping successfully. Unsuccessful pings will transition the service status to FAILED if the Slurm job has failed; TIMEOUT if the Slurm job times out; and UNHEALTHY otherwise.</p> <p>An UNHEALTHY service becomes HEALTHY if the update pings successfully. Otherwise, the service status changes to FAILED if the Slurm job has failed or TIMEOUT if the Slurm job times out.</p> <p>Services that enter a terminal status (FAILED, TIMEOUT or STOPPED) cannot be re-started.</p>"},{"location":"api/services/#blackfish.server.services.base.Service.start","title":"<code>start(session: AsyncSession, app_config: State, container_options: BaseConfig, job_options: JobConfig) -&gt; None</code>  <code>async</code>","text":"<p>Start the service with provided Slurm job and container options. Assumes running in attached state.</p> <p>Submits a Slurm job request, creates a new database entry and waits for the service to start.</p> <p>Parameters:</p> <ul> <li> <code>container_options</code>               (<code>BaseConfig</code>)           \u2013            <p>a dict containing container options (see ContainerConfig).</p> </li> <li> <code>job_options</code>               (<code>JobConfig</code>)           \u2013            <p>a dict containing job options (see JobConfig).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None.</p> </li> </ul>"},{"location":"api/services/#blackfish.server.services.base.Service.stop","title":"<code>stop(session: AsyncSession, timeout: bool = False, failed: bool = False) -&gt; None</code>  <code>async</code>","text":"<p>Stop the service. Assumes running in attached state.</p> <p>The default terminal state is STOPPED, which indicates that the service was stopped normally. Use the <code>failed</code> or <code>timeout</code> flags to indicate that the service stopped due to a Slurm job failure or timeout, resp.</p> <p>This process updates the database after stopping the service.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>flag indicating the service timed out.</p> </li> <li> <code>failed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>flag indicating the service Slurm job failed.</p> </li> </ul>"},{"location":"api/services/#blackfish.server.services.text_generation","title":"<code>blackfish.server.services.text_generation</code>","text":""},{"location":"api/services/#blackfish.server.services.text_generation.TextGeneration","title":"<code>TextGeneration</code>","text":"<p>               Bases: <code>Service</code></p> <p>A containerized service running a text-generation API.</p>"},{"location":"api/services/#blackfish.server.services.text_generation.TextGenerationParameters","title":"<code>TextGenerationParameters</code>  <code>dataclass</code>","text":"<p>Refer to the vLLM docs more information.</p>"},{"location":"api/services/#blackfish.server.services.speech_recognition","title":"<code>blackfish.server.services.speech_recognition</code>","text":""},{"location":"api/services/#blackfish.server.services.speech_recognition.SpeechRecognition","title":"<code>SpeechRecognition</code>","text":"<p>               Bases: <code>Service</code></p> <p>A containerized service running a speech recognition API.</p> <p>Examples:</p> <pre><code>svc = SpeechRecognition(...)\nres = svc(\"/audio/test.mp3\")\n</code></pre>"},{"location":"contrib/developer_guide/","title":"Developer Guide","text":""},{"location":"contrib/developer_guide/#setup","title":"Setup","text":""},{"location":"contrib/developer_guide/#uv","title":"uv","text":"<p><code>uv</code> is optional, but highly recommended. To install Blackfish for development with <code>uv</code> run: <pre><code>git clone https://github.com/princeton-ddss/blackfish.git\ncd blackfish\nuv sync\n</code></pre></p>"},{"location":"contrib/developer_guide/#pre-commit","title":"pre-commit","text":"<p>You should install the <code>pre-commit</code> script: <code>uv run pre-commit install</code>.</p>"},{"location":"contrib/developer_guide/#nox","title":"nox","text":"<p>The development dependencies include <code>nox</code>, which you can use to lint and test code locally: <pre><code>uv run nox\n</code></pre></p>"},{"location":"contrib/developer_guide/#ssh","title":"ssh","text":"<p>Running Blackfish from your laptop to start remote services requires a seamless (i.e., password-less) method of communication with remote clusters. A simple to set up password-less login is with the <code>ssh-keygen</code> and <code>ssh-copy-id</code> utilitites.</p> <p>First, make sure that you are connected to your institution's network or VPN, if required. Then, type the following at the command-line: <pre><code>ssh-keygen -t rsa # generates ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa\nssh-copy-id &lt;user&gt;@&lt;host&gt; # answer yes to transfer the public key\n</code></pre> These commands create a secure public-private key pair and send the public key to the HPC server. You now have password-less access to your HPC server!</p>"},{"location":"contrib/developer_guide/#apptainer","title":"Apptainer","text":"<p>Services deployed on high-performance computing systems need to be run by Apptainer instead of Docker. Apptainer will not run Docker images directly. Instead, you need to convert Docker images to SIF files. For images hosted on Docker Hub, running <code>apptainer pull</code> will do this automatically. For example,</p> <pre><code>apptainer pull docker://ghcr.io/vllm/vllm-openai:latest\n</code></pre> <p>This command generates a file <code>text-generation-inference_latest.sif</code>. In order for users of the remote to access the image, it should be moved to a shared cache directory, e.g., <code>/scratch/gpfs/.blackfish/images</code>.</p>"},{"location":"contrib/developer_guide/#hugging-face","title":"Hugging Face","text":"<p>Update The recommended method to manage models is now via the <code>blackfish model</code> commands using a profile linked to the shared cache directory (make sure to use the <code>--use_cache</code> flag). This will ensure that <code>info.json</code> files are updated. If the shared cache permissions have been set properly, then there should be no need to update permissions on the newly added files.</p> <p>Models should generally be pulled from the Hugging Face Model Hub. This can be done by either visiting the web page for the model card or using of one Hugging Face's Python packages. The latter is preferred as it stores files in a consistent manner in the cache directory. E.g., <pre><code>from transformers import pipeline\npipeline(\n    task='text-generation',\n    model='meta-llama/Meta-Llama-3-8B',\n    token=&lt;token&gt;,\n    revision=&lt;revision&gt;,\n\n)\n# or\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\nmodel = AutoModelForCausalLM('meta-llama/Meta-Llama-3-8b')\n# or\nfrom huggingface_hub import shapshot_download\nsnapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B\")\n</code></pre> These commands store models files to <code>~/.cache/huggingface/hub/</code> by default. You can modify the directory by setting <code>HF_HOME</code> in the local environment or providing a <code>cache_dir</code> argument (where applicable). After the model files are downloaded, they should be moved to a shared cache directory, e.g., <code>/scratch/gpfs/blackfish/models</code>, and permissions on the new model directory should be updated to <code>755</code> (recursively) to allow all users read and execute.</p>"},{"location":"contrib/developer_guide/#api-development","title":"API Development","text":"<p>Blackfish is Litestar application that is managed using the <code>litestar</code> CLI. You can get help with <code>litestar</code> by running <code>litestar --help</code> at the command line from within the application's home directory. Below are some of the essential tasks.</p>"},{"location":"contrib/developer_guide/#litestar-commands","title":"Litestar Commands","text":""},{"location":"contrib/developer_guide/#run","title":"Run","text":"<pre><code>litestar run  # add --reload to automatically refresh updates during development\n</code></pre>"},{"location":"contrib/developer_guide/#database","title":"Database","text":"<pre><code># First, check where your current migration:\nlitestar database show-current-revision\n# Make some updates to the database models, then:\nlitestar database make-migration \"a new migration\"  # create a new migration\n# check that the auto-generated migration file looks correct, then:\nlitestar database upgrade\n</code></pre>"},{"location":"contrib/developer_guide/#configuration","title":"Configuration","text":"<p>The application and command-line interface (CLI) pull their settings from environment variables and/or (for the application) arguments provided at start-up. The environment variables include: <pre><code>HOST = \"localhost\"\nPORT = 8000\nSTATIC_DIR = \"/Users/colinswaney/GitHub/blackfish/src\" # source of static files\nHOME_DIR = \"/Users/colinswaney/.blackfish\" # source of application data\nDEBUG = true # run server in development mode (no auth)\nCONTAINER_PROVIDER = \"docker\" # determines how to launch containers\n</code></pre></p>"},{"location":"contrib/developer_guide/#ui-updates","title":"UI Updates","text":"<p>Blackfish ships with a copy of the built user interface so that users can run the user interface with having to install <code>npm</code>. To update the UI, you need:</p> <ol> <li>Build the UI Run <code>npm run build</code> in the <code>blackfish-ui</code> repo. The output of this command will be in <code>build/out</code>: <pre><code>\u279c tree build -d 1\nbuild\n\u2514\u2500\u2500 out\n    \u251c\u2500\u2500 _next\n    \u2502   \u251c\u2500\u2500 ssm_XfrOvugkYGVtNQ8ps\n    \u2502   \u2514\u2500\u2500 static\n    \u2502       \u251c\u2500\u2500 chunks\n    \u2502       \u2502   \u251c\u2500\u2500 app\n    \u2502       \u2502   \u2502   \u251c\u2500\u2500 _not-found\n    \u2502       \u2502   \u2502   \u251c\u2500\u2500 dashboard\n    \u2502       \u2502   \u2502   \u251c\u2500\u2500 login\n    \u2502       \u2502   \u2502   \u251c\u2500\u2500 speech-recognition\n    \u2502       \u2502   \u2502   \u2514\u2500\u2500 text-generation\n    \u2502       \u2502   \u2514\u2500\u2500 pages\n    \u2502       \u251c\u2500\u2500 css\n    \u2502       \u251c\u2500\u2500 media\n    \u2502       \u2514\n</code></pre></li> <li> <p>Copy <code>blackfish-ui/build/out</code> to <code>blackfish/src/build</code> <pre><code>cp -R build/out/* ~/GitHub/blackfish/src/build\n</code></pre></p> </li> <li> <p>Commit the change <pre><code>git add .\ngit commit\n# Add a useful message that includes the head of the UI, e.g.,\n# Update UI to blackfish-ui@7943376\n</code></pre></p> </li> </ol>"},{"location":"contrib/guidelines/","title":"Blackfish Contributing Guidelines","text":"<p>All  contributions are welcome as long as everyone involved is treated with respect.</p>"},{"location":"contrib/guidelines/#code-contributions","title":"Code Contributions","text":"<p>For advice on setting up your development environment, see our development guide.</p>"},{"location":"contrib/guidelines/#steps","title":"Steps","text":"<ol> <li>Clone the repository and create a feature branch. If you're addressing an open issue, create a feature branch from the issue. Otherwise, give the feature branch a descriptive name (e.g., <code>support-hdf5-files</code>).</li> <li>Write some tests. If fixing a bug, write a test and confirm that it fails. If adding a feature, write appropriate tests to check functionality. Make sure <code>pre-commit</code> is installed. If not, your PR is very likely to fail.</li> <li>Implement your fix or feature. The fun part! Don't be afraid to ask for help or advice.</li> <li>Open a GitHub Pull Request to the <code>master</code> branch. If the PR closes an issue, make sure to note this in the description (e.g., \"Closes #78\").</li> </ol>"},{"location":"contrib/guidelines/#code-review","title":"Code Review","text":"<p>In addition to passing automated tests, PRs must pass code review before they are merged. Feedback will include optional changes and required changes. Required changes must be addressed in order for the PR to be merged. If you disagree with required changes, you can argue your position respectfully, but understand that maintainers have the final say. Do not be discouraged or frustrated with reviewers. Remember: feedback is a GIF!</p> <p></p>"},{"location":"contrib/guidelines/#early-feedback","title":"Early Feedback","text":"<p>Speaking of feedback, getting early feedback is one way to avoid wasted effort and disappointment. There are two ways to request feedback on your ideas. First, you can create an issue describing the issue you wish to resolve or feature that you want to contribute. This is a good option if you are not sure about how to approach the issue and want to avoid heading down the wrong path. The other option is to open a draft PR. This option works well if you know what you're doing (you have an implementation of some sort), but would like a second opinion before you get too far (remember the 90/10 rule!). Additionally, it means the maintainers have one less issue to worry about \ud83d\ude0a</p>"},{"location":"contrib/guidelines/#code-style","title":"Code Style","text":"<p>Code style (linting) and formatting is enforced by <code>ruff</code> and included in the repository's <code>pre-commit</code> configuration. The same <code>pre-commit</code> hooks are run automatically on all PR requests, so developers should install <code>pre-commit</code> to avoid unnecessary failed GitHub Actions.</p>"},{"location":"contrib/guidelines/#docstrings","title":"Docstrings","text":"<p>Provide docstrings for functions, methods, and classes for which the behavior is not obvious. We try to follow the docstring formatting rules from the Google Python Style Guide.</p>"},{"location":"contrib/guidelines/#type-hints","title":"Type Hints","text":"<p>Inclusion of type hints is checked by <code>mypy</code> as part of our <code>pre-commit</code> and GitHub Actions configuration.</p>"},{"location":"contrib/guidelines/#documentation-contributions","title":"Documentation Contributions","text":"<p>Improving documentation is a great way to contribute to Blackfish. Our documentation is nearly always in need to attention. You'll find all our documentation in the <code>docs/</code> directory. We use <code>mkdocs-material</code> for documentation. You can preview changes by installing all <code>dev</code> dependencies (<code>uv sync --dev</code>) and running <code>mkdocs serve</code>.</p>"},{"location":"contrib/guidelines/#bug-reports","title":"Bug Reports","text":"<p>We encourage users to report bugs by creating an issue on GitHub and labeling it as a bug fix. Before raising an issue, please check for duplicate issues.</p>"},{"location":"contrib/guidelines/#feature-requests","title":"Feature Requests","text":"<p>Please let us know if you think Blackfish is missing an important feature by creating an issue on GitHub and labeling it as a feature request.</p>"},{"location":"examples/speech_recognition/basic_transcription/","title":"Basic Transcription","text":"<p>Work in progress \ud83d\udea7 \ud83c\udfd7\ufe0f \ud83e\uddba</p>"},{"location":"examples/text_generation/image_classification/","title":"Image Classification","text":"<p>Work in progress \ud83d\udea7 \ud83c\udfd7\ufe0f \ud83e\uddba</p>"},{"location":"examples/text_generation/prompt_engineering/","title":"Prompt Engineering","text":"<p>Work in progress \ud83d\udea7 \ud83c\udfd7\ufe0f \ud83e\uddba</p>"},{"location":"examples/text_generation/record_linkage/","title":"Record Linkage","text":"<p>Work in progress \ud83d\udea7 \ud83c\udfd7\ufe0f \ud83e\uddba</p>"},{"location":"openapi/swagger/","title":"REST API","text":""},{"location":"setup/installation/","title":"Installation Guide","text":"<p>If your HPC administrator has added Blackfish Ondemand to your Open Ondemand portal, then you can already use Blackfish on your cluster\u2014congratulations! \ud83c\udf89 Otherwise, if Blackfish Ondemand has not been setup, or if you would like to use Blackfish from your laptop, follow the instructions below.</p> <p>Note that Blackfish does not need to be installed on your HPC cluster in order to run services on the cluster. However, if you want to run Blackfish on a login node, it will need to be installed for your cluster account as well.</p> <p>Note</p> <p>Blackfish installations on different machines do not synchronize application data. In particular, Blackfish running on your laptop does not know about services started by Blackfish running on your HPC cluster.</p>"},{"location":"setup/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"setup/installation/#supported-platforms","title":"Supported Platforms","text":"<p>Blackfish is tested on Linux and macOS. Mileage may vary on Windows machines.</p>"},{"location":"setup/installation/#container-provider","title":"Container Provider","text":"<p>In order facilitate reproducibility and minimize dependencies, Blackfish uses Docker and Apptainer to run service containers. HPC-based services require Apptainer; local services support both Docker and Apptainer, but Apptainer only runs on Linux systems.</p>"},{"location":"setup/installation/#ssh-configuration","title":"SSH Configuration","text":"<p>Using Blackfish from your laptop requires a seamless (i.e., password-less) method of communicating with remote clusters. On many systems, this is simple to setup with the <code>ssh-keygen</code> and <code>ssh-copy-id</code> utilitites. First, make sure that you are connected to your institution's network (or VPN), then type the following at the command-line:</p> <pre><code>ssh-keygen -t rsa # generates ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa\nssh-copy-id &lt;user&gt;@&lt;host&gt; # answer yes to transfer the public key\n</code></pre> <p>These commands create a secure public-private key pair and send the public key to the HPC server you need access to. You now have password-less access to your HPC server!</p> <p>Warning</p> <p>Blackfish depends on seamless interaction with your university's HPC cluster. Before proceeding, make sure that you have enabled password-less login and are connected to your institutions network or VPN, if required.</p>"},{"location":"setup/installation/#installation","title":"Installation","text":""},{"location":"setup/installation/#pip","title":"pip","text":"<pre><code>python -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install blackfish-ai\n</code></pre>"},{"location":"setup/installation/#uv","title":"uv","text":"<pre><code>uv venv\nuv add blackfish-ai\n</code></pre>"},{"location":"setup/installation/#initialization","title":"Initialization","text":"<p>Before you use Blackfish for the first time, you need to initialize it:</p> <pre><code>blackfish init\n</code></pre> <p>Answer the prompts to create a new default profile. If you are setting up Blackfish on a cluster, your default profile should be a \"local\" profile:</p> <pre><code># &gt; name: default\n# &gt; type: local\n# &gt; user: shamu\n# &gt; home: /home/shamu/.blackfish\n# &gt; cache: /shared/.blackfish\n</code></pre> <p>If you are installing Blackfish on your laptop, then you probably want your default profile to be a \"Slurm\" profile:</p> <pre><code># &gt; name: default\n# &gt; type: slurm\n# &gt; host: cluster.organization.edu\n# &gt; user: shamu\n# &gt; home: /home/shamu/.blackfish\n# &gt; cache: /shared/.blackfish\n</code></pre> <p>The home directory supplied must be a directory for which your user has read-write permissions; the cache directory only requires read permissions.</p> <p>Note</p> <p>If your default profile connects to an HPC cluster, Blackfish will attempt to set up the remote host at this point. Profile creation will fail if you're unable to connect to the HPC server and you'll need to re-run the <code>blackfish init</code> command or create a profile with <code>blackfish profile create</code>.</p>"},{"location":"setup/management/","title":"Management Guide","text":"<p>This guide explains how to perform tasks to ensure that Blackfish has access to everything it needs in order to run services. If you are using Blackfish Ondemand, these steps have already been taken care of by your system admin.</p> <p>If you are a system admin, or you do not have access to Blackfish Ondemand, these notes are for you.</p>"},{"location":"setup/management/#architecture-overview","title":"Architecture Overview","text":"<p>Blackfish consists of four components: a core REST API, a command-line interface (CLI), a browser-based user interface (UI), and a Python API. The core REST API performs all key service management operations while the Blackfish CLI and UI provide convenient methods for interacting with the Blackfish API. The Python API allows researchers to use Blackfish within Python scripts and pipelines.</p> <p>The Blackfish REST API automates the process of hosting AI models as APIs. Users instruct the Blackfish API  via the CLI or UI to deploy a model and the REST API creates a \"service API\" running that model. The researcher that starts a service \"owns\" that service: she has exclusive access to its use. Blackfish tracks the status of the users' services and provides methods to stop and delete services when they are no longer needed.</p> <p>In general, service APIs do not run on the same machine as the Blackfish application. Thus, when a researcher requests a model, she must specify a host for the service. The service API runs on the specifieid host and Blackfish ensures that the interface is able to communicate with the remote service API. There are several ways for researchers to setup and use Blackfish depending on their requirements. For testing and development purposes, users can run everything on their laptop, but his option is only practical for models with light resource requirements. Typically, users will want to run services on high-performance GPUs available on an HPC cluster with a Slurm job scheduler. In that case, researchers can run the Blackfish API on their local laptop or on an HPC login node.</p> <p>Note</p> <p>Blackfish doesn't synchronize application data across machines. Services started by an instance of Blackfish running on your laptop will not show up on an HPC cluster. However, job data for services initiated by your laptop will be stored on the remote cluster.</p> <p></p> <p>Figure 1 The Blackfish architecture for running remote services on a Slurm cluster.</p>"},{"location":"setup/management/#application-data","title":"Application Data","text":"<p>Blackfish stores data in several different locations:</p> <ul> <li>Core application data is stored in <code>BLACKFISH_HOME_DIR</code> on the system where Blackfish is running (<code>~/.blackfish</code> by default). Core application data includes profile configuration, application logs, and database storage.</li> <li>Models and images are stored in the user-defined locations <code>home_dir</code> and <code>cache_dir</code>. These are profile-specific locations that need not reside on the machine where Blackfish is running. <code>home_dir</code> also stores job files created each time a service launches.</li> </ul> <p>Let's consider what happens when a user launches a service from her laptop targeting a remote HPC cluster (Figure 1). The user will specify a profile that tells Blackfish the <code>host</code> and <code>user</code> of the targeted cluster. Blackfish will use this information to look for required model and image files in either the <code>home_dir</code> or <code>cache_dir</code>\u2014also specified by the profile\u2014on the cluster. If the required files exist, Blackfish creates a Slurm job script, stores it in <code>$BLACKFISH_HOME_DIR/jobs/$service_id</code>, then copies that job script to <code>$home_dir/jobs/$service_id</code> on the remote cluster. Finally, Blackfish remotely submits the Slurm job and stores its log files to <code>$home_dir/jobs/$service_id</code>.</p>"},{"location":"setup/management/#images","title":"Images","text":"<p>Blackfish does not ship with the Docker (OCI) or Apptainer (SFI) container images required to run services. These images should be downloaded before running services<sup>1</sup>. The current required images are:</p> <ul> <li>Text generation: <code>vllm/vllm-openai:v0.8.4</code></li> <li>Speech recognition: <code>princeton-ddss/speech-recognition-inference:0.1.2</code></li> </ul> <p>These images are expected to change over time, so be sure to check release notes for updates.</p>"},{"location":"setup/management/#obtaining-images","title":"Obtaining Images","text":""},{"location":"setup/management/#apptainer","title":"Apptainer","text":"<p>Services deployed on high-performance computing systems should be run with Apptainer instead of Docker. Apptainer requires Single Image Format (SIF) images instead of the Open Container Image (OCI) format used by Docker. Thus, Docker images must be converted to SIF files before Blackfish can use them. For most images\u2014including those hosted on the GitHub container registry\u2014running <code>apptainer pull</code> will do this automatically. For example,</p> <pre><code>apptainer pull docker://ghcr.io/princeton-ddss/speech-recognition-inference:0.1.2\n</code></pre> <p>This command generates a file <code>speech-recognition-inference_0.1.2.sif</code> in the directory where it is run. If you are setting up Blackfish for your own account, you should move this image to your home directory, <code>/home/shamu/.blackfish/image</code>. If you are setting up a shared Blackfish environment, move the image to a shared cache directory, e.g., <code>/shared/.blackfish/images</code>.</p>"},{"location":"setup/management/#docker","title":"Docker","text":"<p>For local service deployment, Docker handles file management, so you can simply pull the required image, e.g.:</p> <pre><code>docker pull vllm/vllm-openai:v0.8.4\n</code></pre>"},{"location":"setup/management/#models","title":"Models","text":""},{"location":"setup/management/#automatic-downloads","title":"Automatic Downloads","text":"<p>You can download models with the <code>blackfish model add</code> command. Blackfish stores downloaded models to the <code>home_dir</code> of the specified profile by default. If you are downloading models to share with other users, add the <code>--use-cache</code> flag to save files to the <code>cache_dir</code> instead. Model download support is currently limited to local profiles. If you want to download models for use on HPC, you'll need to be running Blackfish on your cluster.</p>"},{"location":"setup/management/#manual-downloads","title":"Manual Downloads","text":"<p>Internally, model downloads and management are performed by <code>huggingface_hub</code>. You can download models yourself using the same method:</p> <pre><code>from huggingface_hub import shapshot_download\nsnapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B\")\n</code></pre> <p>The <code>snapshot_download</code> method store models files to <code>~/.cache/huggingface/hub/</code> by default. You should modify the directory by setting <code>HF_HOME</code> in the local environment or providing a <code>cache_dir</code> argument. Otherwise, after the model files are downloaded, they must be manually moved to your home or shared (cache) directory, e.g., <code>/shared/.blackfish/models</code>. For shared models, remember to set permissions on the model directory to <code>755</code> (recursively) to allow all users read and execute access.</p> <p>Note</p> <p>In addition to downloading model files, the <code>blackfish model add</code> command extracts metadata from the model and adds it go an internal database of models available to the profile that was used to add the model. Manually added models will not show up when running <code>blackfish model ls</code> (because they are not added to this database), but Blackfish will still be able to discover and run these models.</p> <ol> <li> <p>If you only intend to run services on your laptop, Blackfish will attempt to download each image automatically the first time you run its corresponding service. In this case, expect the startup time for the first run of each service type to take much longer than subsequent runs.\u00a0\u21a9</p> </li> </ol>"},{"location":"usage/cli/","title":"Command Line","text":"<p>This guide walks through usage of the Blackfish command line tool (CLI). In the current version of Blackfish, the CLI is the only way to perform certain management operations, such as creating profiles and adding models. Thus, it's highly recommended that users develop some level of familiarity with the CLI even if intend to primarily use the UI or Python API<sup>1</sup>.</p>"},{"location":"usage/cli/#configuration","title":"Configuration","text":"<p>The Blackfish application (i.e., REST API) and command-line interface (CLI) pull settings from environment variables and/or (for the application) arguments provided at start-up. The most important environment variables are:</p> <ul> <li><code>BLACKFISH_HOST</code>: host for local instance of the Blackfish app (default: <code>'localhost'</code>)</li> <li><code>BLACKFISH_PORT</code>: port for local instance of the Blackfish app (default: <code>8000</code>)</li> <li><code>BLACKFISH_HOME_DIR</code>: location to store application data (default: <code>'~/.blackfish'</code>)</li> <li><code>BLACKFISH_DEBUG</code>: whether to run the application in debug mode (default: <code>True</code>)</li> <li><code>BLACKFISH_AUTH_TOKEN</code>: a user-defined secret authentication token. Ignored if <code>DEBUG=True</code>.</li> </ul> <p>Running the application in debug mode is recommended for development only on shared systems as it does not use authentication. In \"production mode\", Blackfish randomly generates an authentication token.</p> <p>Note</p> <p>The settings for the REST API are determined when the Blackfish application is started via <code>blackfish start</code>. Subsequent interactions with the API via the command line assume that the CLI is using the same configuration and will fail if this is not the case. For example, if you start Blackfish with <code>BLACKFISH_PORT=8081</code> and then try to run commands in a new terminal where <code>BLACKFISH_PORT</code> isn't set, the CLI will not be able communicate with the API.</p>"},{"location":"usage/cli/#profiles","title":"Profiles","text":"<p>Blackfish's primary function is to launch services that perform AI tasks. These services are, in general, detached from the system Blackfish runs on. Thus, we require a method to instruct Blackfish how we want to run services: what cluster should Blackfish use, and where should it look for any resources it needs? Profiles are Blackfish's method of saving this information and applying it across commands. A default profiles is required, but multiple profiles are useful if you have access to multiple HPC resources or have multiple accounts on a single HPC cluster.</p> <p>Tip</p> <p>Blackfish profiles are stored in <code>$BLACKFISH_HOME/profiles.cfg</code>. On Linux, this is <code>$HOME/.blackfish/profiles.cfg</code> by default. You can modify this file directly, if needed, but you'll need to need setup any required remote resources by hand.</p>"},{"location":"usage/cli/#schemas","title":"Schemas","text":"<p>Every profile specifies a number of attributes that allow Blackfish to find resources (e.g., model files) and deploy services accordingly. The exact attributes depend on the profile schema. There are currently two profile schemas: <code>LocalProfile</code> (\"local\") and <code>SlurmProfile</code> (\"slurm\"). All profiles require the following attributes:</p> <ul> <li><code>name</code>: a unique profile name. The profile named \"default\" is used by Blackfish when a profile isn't explicitly provided.</li> <li><code>schema</code>: one of \"slurm\" or \"local\". The profile schema determines how services associated with this profile are deployed by Blackfish. Use \"slurm\" if this profile will run jobs on an HPC cluster (via a Slurm job scheduler) and \"local\" to run services on your laptop.</li> </ul> <p>The additional attribute requirements for specific types are listed below.</p>"},{"location":"usage/cli/#local","title":"Local","text":"<p>A local profile specifies how to run services on a local machine, i.e., your laptop or desktop, without a job scheduler. This is useful for development and running models that do not require significant resources, especially if the model is able to use the GPU on your laptop.</p> <ul> <li><code>home_dir</code>: a user-owned location to store model and image files on the local machine, e.g., <code>/home/&lt;user&gt;/.blackfish</code>. User should have read-write access to this directory.</li> <li><code>cache_dir</code>: a shared location to source model and image files from. Blackfish does not attempt to create this directory for you, but it does require that it can be found. User should at least have read access to this directory.</li> </ul>"},{"location":"usage/cli/#slurm","title":"Slurm","text":"<p>A Slurm profile specifies how to schedule services on a (possibly) remote server (e.g., HPC cluster) running Slurm.</p> <ul> <li><code>host</code>: a server to run services on, e.g. <code>&lt;cluster&gt;@&lt;university&gt;.edu</code> or <code>localhost</code> if also running Blackfish on the cluster.</li> <li><code>user</code>: a user name used to connect to server.</li> <li><code>home</code>: a location on the server to store application model, image and job data, e.g., <code>/home/&lt;user&gt;/.blackfish</code>. User should have read-write access to this directory.</li> <li><code>cache</code>: a location on the server to source additional shared model and images files from. Blackfish does not attempt to create this directory for you, but it does require that it can be found. User should at least have read access to this directory.</li> </ul>"},{"location":"usage/cli/#managing-profiles","title":"Managing Profiles","text":"<p>The <code>blackfish profile</code> command provides methods for managing Blackfish profiles.</p>"},{"location":"usage/cli/#ls-list-profiles","title":"ls - List profiles","text":"<p>To view all profiles, type</p> <pre><code>blackfish profile ls\n</code></pre>"},{"location":"usage/cli/#add-create-a-profile","title":"add - Create a profile","text":"<p>Creating a new profile is as simple as typing</p> <pre><code>blackfish profile add\n</code></pre> <p>and following the prompts (see attribute descriptions above). Note that profile names are unique.</p>"},{"location":"usage/cli/#show-view-a-profile","title":"show - View a profile","text":"<p>You can view a list of all profiles with the <code>blackfish profile ls</code> command. If you want to view a specific profile, use the <code>blackfish profile show</code> command instead, e.g.</p> <pre><code>blackfish profile show --name &lt;profile&gt;\n</code></pre> <p>Leaving off the <code>--name</code> option above will display the default profile, which is used by most commands if no profile is explicitly provided.</p>"},{"location":"usage/cli/#update-modify-a-profile","title":"update - Modify a profile","text":"<p>To modify a profile, use the <code>blackfish profile update</code> command, e.g.</p> <pre><code>blackfish profile update --name &lt;profile&gt;\n</code></pre> <p>This command updates the default profile if not <code>--name</code> is specified. Note that you cannot change the <code>name</code> or <code>schema</code> attributes of a profile.</p>"},{"location":"usage/cli/#rm-delete-a-profile","title":"rm - Delete a profile","text":"<p>To delete a profile, type <code>blackfish profile rm --name &lt;profile&gt;</code>. By default, the command requires you to confirm before deleting.</p> <pre><code>blackfish profile rm --name &lt;profile&gt;\n</code></pre>"},{"location":"usage/cli/#services","title":"Services","text":"<p>Once you've initialized Blackfish and created a profile, you're ready to get to work! The entrypoint for working with the blackfish CLI is to type</p> <pre><code>blackfish start\n</code></pre> <p>in your terminal. If everything worked, you should see a message stating the application startup is complete. This command starts the Blackfish API and UI. At this point, you're free to switch over to the UI, if desired: just mosey on over to <code>http://localhost:8000</code> in your favorite browser. It's a relatively straight-forward interface, and we provide a detailed usage guide. But let's stay focus on the CLI.</p> <p>Open a new terminal tab or window. First, let's see what type of services are available.</p> <pre><code>blackfish run --help\n</code></pre> <p>This command displays a list of available sub-commands. One of these is <code>text-generation</code>, which is a service that generates text given an input prompt. There are a variety of models that we might use to perform this task, so let's check out what's available on our setup.</p>"},{"location":"usage/cli/#obtaining-models","title":"Obtaining Models","text":"<p>The command to list available models is:</p> <pre><code>blackfish model ls\n</code></pre> <p>Once you've added some models or if you already have access a shared cache directory of model, the output should look something like the following:</p> <pre><code>REPO                                   REVISION                                   PROFILE   IMAGE\nopenai/whisper-tiny                    169d4a4341b33bc18d8881c4b69c2e104e1cc0af   default   speech-recognition\nopenai/whisper-tiny                    be0ba7c2f24f0127b27863a23a08002af4c2c279   default   speech-recognition\nopenai/whisper-small                   973afd24965f72e36ca33b3055d56a652f456b4d   default   speech-recognition\nTinyLlama/TinyLlama-1.1B-Chat-v1.0     ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971   default   text-generation\nmeta-llama/Meta-Llama-3-70B            b4d08b7db49d488da3ac49adf25a6b9ac01ae338   macbook   text-generation\nopenai/whisper-tiny                    169d4a4341b33bc18d8881c4b69c2e104e1cc0af   macbook   speech-recognition\nTinyLlama/TinyLlama-1.1B-Chat-v1.0     4f42c91d806a19ae1a46af6c3fb5f4990d884cd6   macbook   text-generation\n</code></pre> <p>As you can see, there are a number of models available<sup>2</sup>. Notice that <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> is listed twice. The first listing refers to a specific \"revision\" (i.e., version) of this model\u2014 <code>ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971</code>\u2014that is available to the <code>default</code> profile; the second listing refers to a different version of the same model\u2014<code>4f42c91d806a19ae1a46af6c3fb5f4990d884cd6</code>\u2014that is available to the <code>macbook</code> profile. For reproducibility, it's important to keep track of the exact revision used.</p> <p>Let's say you would really prefer to use a smaller version of <code>Llama</code> than the 70 billion parameter model shown above, say <code>meta-llama/Meta-Llama-3-1B</code>. To add the new model, we would simply type</p> <pre><code>blackfish model add meta-llama/Meta-Llama-3-1B\n</code></pre> <p>This command downloads the model files, store them to the default profile's <code>home_dir</code>, and updates the model database. Note that the <code>model add</code> command currently only supports local downloads: if your default profile points to a remote cluster, then you'll need to run the command on that server instead.</p> <p>Let's go ahead and run a service using one of these models.</p>"},{"location":"usage/cli/#managing-services","title":"Managing Services","text":"<p>A service is a containerized API that is called to perform a specific task, such a text generation, using a model specified by the user when the API is created. Services perform inference in an \"online\" fashion, meaning that, in general, they process requests one at a time<sup>3</sup>. Users can create as many services as they like (up to resource availability) and interact with them simultaneously. Services are completely managed by the user: as the creator of a service, you can stop or restart the service, and you control access to the service via an authentication token.</p>"},{"location":"usage/cli/#run-start-a-service","title":"<code>run</code> - Start a service","text":"<p>Looking back at the help message for <code>blackfish run</code>, we see that there are a few items that we should provide. First, we need to select the type of service to run. We've already decide to run <code>text-generation</code>, so we're good there. Next, there are a number of job options that we can provide. With the exception of <code>profile</code>, job options are based on the Slurm <code>sbatch</code> command and tell Blackfish the resources required to run a service. Finally, there are a number of \"container options\" available. To get a list of these, type <code>blackfish run text-generation --help</code>:</p> <pre><code>blackfish run text-generation --help\n</code></pre> <p>The most important of these is the <code>revision</code>, which specifies the exact version of the model we want to run. By default, Blackfish selects the most recent locally available version. This container option (as well as <code>--name</code>) is available for all tasks: the remaining options are task-specific.</p> <p>We'll choose <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code> for the required <code>MODEL</code> argument, which we saw earlier is available to the <code>default</code> and <code>macbook</code> profiles. This is a relatively small model, but we still want to ask for a GPU to speed things up. Putting it altogether, here's the command to start your service:</p> <pre><code>blackfish run \\\n  --gres 1 \\\n  --mem 8 \\\n  --ntasks-per-node 4 \\\n  --time 00:30:00 \\\n  text-generation TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n  --api-key sealsaretasty\n</code></pre> <p>Warning</p> <p>Omitting the <code>--api-key</code> argument leaves your service naked. Others users of the system where your service is running could potentially hijack your server or even gain access to your files via the service.</p> <p>If everything worked, you should see output that looks something like this:</p> <pre><code>\u2714 Found 49 models.\n\u2714 Found 1 snapshots.\n\u26a0 No revision provided. Using latest available commit: fe8a4ea1ffedaf415f4da2f062534de366a451e6.\n\u2714 Found model TinyLlama/TinyLlama-1.1B-Chat-v1.0!\n\u2714 Started service: fed36739-70b4-4dc4-8017-a4277563aef9\n</code></pre> <p>What just happened? First, Blackfish checked to make sure that the requested model is available to the <code>default</code> profile. Next, it found a list of available revisions of the model and selected the most recently published version because no revision was specified. Finally, it sent a request to deploy the model. Helpfully, the CLI returned an ID associated with the new service <code>fed36739-70b4-4dc4-8017-a4277563aef9</code>, which you can use get information about our service via the <code>blackfish ls</code> command.</p> <p>Note</p> <p>If no <code>--revision</code> is provided, Blackfish automatically selects the most recently available downloaded version of the requested model. This reduces the time-to-first-inference, but may not be desirable for your use case. Download the model before starting your service if you need the most recent version available on Hugging Face.</p> <p>Tip</p> <p>Add the <code>--dry-run</code> flag to preview the start-up script that Blackfish will submit.</p>"},{"location":"usage/cli/#ls-list-services","title":"<code>ls</code> - List services","text":"<p>To view a list of your running Blackfish services, type</p> <pre><code>blackfish ls # --filter id=&lt;service_id&gt;,status=&lt;status&gt;\n</code></pre> <p>This will output a table similar to the following:</p> <pre><code>SERVICE ID      IMAGE                MODEL                                CREATED       UPDATED     STATUS    PORT   NAME              PROFILE\n97ffde37-7e02   speech_recognition   openai/whisper-large-v3              7 hours ago   1 min ago   HEALTHY   8082   blackfish-11846   default\nfed36739-70b4   text_generation      TinyLlama/TinyLlama-1.1B-Chat-v1.0   7 sec ago     5 sec ago   PENDING   None   blackfish-89359   della\n</code></pre> <p>The last item in this list is the service we just started. In this case, the <code>default</code> profile happens to be set up to connect to a remote HPC cluster, so the service is run as a Slurm job. It may take a few minutes for our Slurm job to start, and it will require additional time for the service to be ready after that<sup>4</sup>. Until then, our service's status will be either <code>SUBMITTED</code>, <code>PENDING</code> or <code>STARTING</code>. Now would be a good time to brew a hot beverage \u2615\ufe0f.</p> <p>Tip</p> <p>You can get more detailed information about a service with the <code>blackfish details &lt;service_id&gt;</code> command. Again, <code>--help</code> is your friend if you want more information.</p> <p>Now that you're refreshed, let's see how our service is doing. Re-run the command above. If things went smoothly, then you should see that the service's status has changed to <code>HEALTHY</code> (if your service is still <code>STARTING</code>, give it another minute and try again).</p> <p>At this point, we can start interacting with the service. Let's say \"Hello\", shall we?</p> <p>The details of calling a service depend on the service you are trying to connect to. For the <code>text-generation</code> service, the primary endpoint is <code>/v1/chat/completions</code>. Here's a typical request from the command-line:</p> <pre><code>curl http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer sealsaretasty\" \\\n  -d '{\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are an expert marine biologist.\"},\n            {\"role\": \"user\", \"content\": \"Why are orcas so awesome?\"}\n        ],\n        \"max_completion_tokens\": 100,\n        \"temperature\": 0.1,\n        \"stream\": false\n    }' | jq\n</code></pre> <p>A successful response will look like this:</p> <pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1192  100   911  100   281   1652    509 --:--:-- --:--:-- --:--:--  2159\n{\n  \"id\": \"chatcmpl-b6452981728f4f3cb563960d6639f8a4\",\n  \"object\": \"chat.completion\",\n  \"created\": 1747826716,\n  \"model\": \"/data/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"reasoning_content\": null,\n        \"content\": \"Orcas (also known as killer whales) are incredibly intelligent and social animals that are known for their incredible abilities. Here are some reasons why orcas are so awesome:\\n\\n1. Intelligence: Orcas are highly intelligent and have been observed using tools, communicating with each other, and even learning from their trainers.\\n\\n2. Social behavior: Orcas are highly social animals and form complex social structures, including family groups, pods,\",\n        \"tool_calls\": []\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 40,\n    \"total_tokens\": 140,\n    \"completion_tokens\": 100,\n    \"prompt_tokens_details\": null\n  },\n  \"prompt_logprobs\": null\n}\n</code></pre> <p>You can, of course, use any language you like for communicating with services: Python, R, JavaScript, etc. In the case of <code>text-generation</code>, you can also use client-libraries like <code>openai-python</code> to simplify API workflows.</p> <p>Tip</p> <p>The <code>text-generation</code> service runs vLLM's OpenAI-compatible server. If you are used to working with ChatGPT, this API should be familiar and your scripts will generally \"just work\" if you point them to Blackfish instead. <code>vllm serve</code> supports a number of endpoints depending on the arguments provided. Any unrecognized arguments passed to the <code>text-generation</code> command are passed through to <code>vllm serve</code>, allowing users to control the precise deploy details of the <code>vllm</code> server.</p>"},{"location":"usage/cli/#stop-stop-a-service","title":"<code>stop</code> - Stop a service","text":"<p>When you are done with a service, you should shut it down and return its resources to the cluster. To do so, simply type:</p> <pre><code>blackfish stop fed36739-70b4-4dc4-8017-a4277563aef9\n</code></pre> <p>You should receive a nice message stating that the service was stopped, which you can confirm by checking its status with <code>blackfish ls</code>.</p>"},{"location":"usage/cli/#rm-delete-a-service","title":"<code>rm</code> - Delete a service","text":"<p>Blackfish keeps a record of every service that you've ever run. These records aren't automatically cleaned up, so it's a good idea to delete them when you're done using a service (if you don't need them for record keeping):</p> <pre><code>blackfish rm --filters id=fed36739-70b4-4dc4-8017-a4277563aef9\n</code></pre> <ol> <li> <p>Researchers that only intend to use Blackfish OnDemand should not generally need to interact with the CLI.\u00a0\u21a9</p> </li> <li> <p>The list of models displayed depends on your environment. If you do not have access to a shared HPC cache, your list of models is likely empty. Not to worry\u2014we will see how to add models later on. If this is your first time running the command, use the <code>--refresh</code> flag to tell Blackfish to search for models in your cache directories and update the model database.\u00a0\u21a9</p> </li> <li> <p>In practice, services like <code>vLLM</code> can use dynamic batching to process requests concurrently. The number of concurrent requests these service can process is limited by a number of factors including the amount of memory available and properties of the requests themselves.\u00a0\u21a9</p> </li> <li> <p>The bulk of this time is spent loading model weights into memory. For small models (&lt; 1B parameters), the service might be ready in a matter of seconds. Large models (~8B) might take 5-10 minutes to load.\u00a0\u21a9</p> </li> </ol>"},{"location":"usage/client/","title":"Python API","text":"<p>While the Blackfish CLI and UI are convenient for interactive, time-limited, or small-scale projects, they can prove quite awkward for workflows that require automation and/or might run for more than a few hours.</p> <p>For this reason, Blackfish provides a Python API for managing services directly from Python scripts, without requiring the REST API server to be running. This allows users to, for example, define tasks that make requests to a text generation service as part of a larger orchestration script.</p> <p>We provide synchronous and asynchronous APIs.</p>"},{"location":"usage/client/#synchronous-api","title":"Synchronous API","text":"<p>The synchronous API is the simplest way to use Blackfish in Python scripts:</p> <pre><code>from blackfish import Blackfish, ManagedService\n\n# Initialize the client\nbf = Blackfish(debug=True)\n\n# Create a service\nservice = bf.launch_service(\n    name=\"tiny-llama-service\",\n    image=\"text_generation\",\n    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    job_config={\n        \"name\": \"tiny-llama-service\",\n        \"time\": \"01:00:00\",\n        \"mem\": 8,\n        \"gres\": 1,\n    }\n)\n\n# Wait for service\nservice.wait(timeout=300)\nif service.status == \"healthy\":\n    print(\"Yipee!\")\nelse:\n    print(\"Shucks!\")\n\n# List all services\nservices = bf.list_services()\nfor svc in services:\n    print(f\"{svc.id}: {svc.status}\")\n\n# Refresh service status\nservice.refresh()\nif service.status == \"healthy\":\n    print(\"All good!\")\nelse:\n    print(\"Peanuts.\")\n\n# Stop and clean up\nservice.stop()\nservice.delete()\nbf.close()\n</code></pre>"},{"location":"usage/client/#asynchronous-api","title":"Asynchronous API","text":"<p>For async applications, use the async methods (prefixed with <code>async</code>):</p> <pre><code>import asyncio\nfrom blackfish import Blackfish\n\nasync def main():\n    async with Blackfish() as bf:\n        # Create a service\n        service = await bf.async_launch_service(\n            name=\"tiny-llama-service\",\n            image=\"text_generation\",\n            model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n            job_config={\n                \"name\": \"tiny-llama-service\",\n                \"time\": \"01:00:00\",\n                \"mem\": 8,\n                \"gres\": 1,\n            }\n        )\n\n        # List services\n        services = await bf.async_list_services()\n\n        # Stop and clean up\n        await service.async_stop()\n        await service.async_delete()\n\nasyncio.run(main())\n</code></pre>"},{"location":"usage/client/#resource-management","title":"Resource Management","text":""},{"location":"usage/client/#services","title":"Services","text":"<p>Blackfish launches services running on external resources. Generally, you will want to tie the lifetime of services to the lifetime of your Python script to ensure that external resources are released. This is the default behavior for services.</p> <p>In some cases, however, you may want services to outlive your script. To accomplish this, simply set <code>auto_cleanup=False</code>:</p> <pre><code>bf.launch_service(..., auto_cleanup=False)\n</code></pre>"},{"location":"usage/client/#client","title":"Client","text":"<p>Use context managers for automatic cleanup of the Blackfish client:</p> <pre><code># Sync context manager\nwith Blackfish() as bf:\n    service = bf.create_service(...)\n    # Connection closes automatically\n\n# Async context manager\nasync with Blackfish() as bf:\n    service = await bf.acreate_service(...)\n    # Connection closes automatically\n</code></pre> <p>Or manually (with sync API):</p> <pre><code>bf = Blackfish()\n# ... do work ...\nbf.close()\n</code></pre>"},{"location":"usage/client/#service-objects","title":"Service Objects","text":"<p>The <code>ManagedService</code> type wraps a <code>Service</code> that should always point to a service that is tracked by the Blackfish database. This means that Blackfish will not lose track of your service even if your Python session crashes<sup>1</sup>. You can access the internal service's attributes exactly as if you were working with the underlying <code>Service</code>:</p> <pre><code>print(f\"Service: {service.id}\")\nprint(f\"Status: {service.status}\")\nprint(f\"Host: {service.host}\")\nprint(f\"Port: {service.port}\")\n</code></pre> <p>Note</p> <p>The status of a service should be understood as the most recently observed status of that service. The current status of a service only known to the external system running the service, e.g., Slurm. To update the status of a service, use <code>service.refresh()</code>.</p> <p>After a service is deleted, however, the internal service is no longer valid and is therefore set to <code>None</code>. At this point, attempting to access the service's attributes will produce a runtime error:</p> <pre><code>&gt;&gt;&gt; service.delete()\n\u2714 Service deleted: 20054b7c-7600-4c4e-9dde-d893333ca8b1\nTrue\n&gt;&gt;&gt; service.id\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 s.status\n\nFile ~/GitHub/blackfish/src/blackfish/__init__.py:83, in ManagedService.__getattr__(self, name)\n     81 \"\"\"Delegate attribute access to the underlying service.\"\"\"\n     82 if self._service is None:\n---&gt; 83     raise RuntimeError(\"This service has been deleted and can no longer be accessed.\")\n     84 return getattr(self._service, name)\n\nRuntimeError: This service has been deleted and can no longer be accessed.\n</code></pre>"},{"location":"usage/client/#examples","title":"Examples","text":""},{"location":"usage/client/#monitoring-services","title":"Monitoring Services","text":"<pre><code>from blackfish import Blackfish, ServiceStatus\n\ndef monitor_services(bf: Blackfish):\n    \"\"\"Print status of all active services.\"\"\"\n    services = bf.list_services()\n\n    active = [s for s in services if s.status == ServiceStatus.HEALTHY]\n\n    print(f\"Active services: {len(active)}\")\n    for service in active:\n        print(f\"  &gt; {service.id}: {service.host}:{service.port}\")\n\nwith Blackfish() as bf:\n    monitor_services(bf)\n</code></pre>"},{"location":"usage/client/#concurrent-service-creation-async","title":"Concurrent Service Creation (Async)","text":"<pre><code>import asyncio\nfrom blackfish import Blackfish\n\nasync def create_multiple_services():\n    async with Blackfish() as bf:\n        tasks = [\n            bf.async_create_service(\n                name=f\"service-1\",\n                image=\"text_generation\",\n                model=\"meta-llama/Llama-3.3-70B-Instruct\",\n                profile_name=\"default\"\n            ),\n            bf.async_create_service(\n                name=f\"service-2\",\n                image=\"text_generation\",\n                model=\"meta-llama/Llama-3.3-70B-Instruct\",\n                profile_name=\"default\"\n            ),\n        ]\n\n        services = await asyncio.gather(*tasks)\n        print(f\"Created {len(services)} services\")\n\n        for service in services:\n            print(f\"  {service.name}: {service.id}\")\n\nasyncio.run(create_multiple_services())\n</code></pre>"},{"location":"usage/client/#troubleshooting","title":"Troubleshooting","text":""},{"location":"usage/client/#service-wont-start","title":"Service won't start","text":"<p>Enable debug logging and check the service status:</p> <pre><code>from blackfish import set_logging_level\n\nset_logging_level(\"debug\")\n\nservice = bf.get_service(service_id)\nprint(f\"Status: {service.status}\")\n\njob = service._service.get_job(verbose=True)\nif job:\n    print(f\"Job state: {job.state}\")\n</code></pre>"},{"location":"usage/client/#database-connection-issues","title":"Database connection issues","text":"<p>Ensure Blackfish is initialized:</p> <pre><code>blackfish init\n</code></pre> <p>And verify that the home directory <code>~/.blackfish</code> exists.</p> <ol> <li> <p>If <code>auto_clean=False</code>. Otherwise, Blackfish automatically deletes services on shutdown.\u00a0\u21a9</p> </li> </ol>"},{"location":"usage/ui/","title":"Web Interface","text":"<p>Work in progress \ud83d\udea7 \ud83c\udfd7\ufe0f \ud83e\uddba</p>"}]}